[{'text': 'Morning and welcome back.', 'start': 3.59, 'duration': 3.25}, {'text': "So what we'll see today in class", 'start': 6.84, 'duration': 4.125}, {'text': 'is the first in-depth discussion of a learning algorithm,', 'start': 10.965, 'duration': 3.795}, {'text': 'linear regression, and in particular,', 'start': 14.76, 'duration': 2.16}, {'text': 'over the next, what,', 'start': 16.92, 'duration': 1.425}, {'text': "hour and a bit you'll see linear regression,", 'start': 18.345, 'duration': 3.555}, {'text': 'batch and stochastic gradient descent is', 'start': 21.9, 'duration': 2.16}, {'text': 'an algorithm for fitting linear regression models,', 'start': 24.06, 'duration': 1.98}, {'text': 'and then the normal equations, um, uh,', 'start': 26.04, 'duration': 3.315}, {'text': 'as a way of- as a very efficient way to let you fit linear models.', 'start': 29.355, 'duration': 5.43}, {'text': "Um, and we're going to define notation,", 'start': 34.785, 'duration': 3.315}, {'text': 'and a few concepts today that will lay the foundation', 'start': 38.1, 'duration': 3.229}, {'text': "for a lot of the work that we'll see the rest of this quarter.", 'start': 41.329, 'duration': 3.526}, {'text': "Um, so to- to motivate linear regression, it's gonna be, uh,", 'start': 44.855, 'duration': 3.735}, {'text': 'maybe the- maybe the simplest,', 'start': 48.59, 'duration': 1.65}, {'text': 'one of the simplest learning algorithms.', 'start': 50.24, 'duration': 1.86}, {'text': 'Um, you remember the ALVINN video,', 'start': 52.1, 'duration': 3.24}, {'text': 'the autonomous driving video that I had shown in class on Monday,', 'start': 55.34, 'duration': 3.9}, {'text': 'um, for the self-driving car video,', 'start': 59.24, 'duration': 2.345}, {'text': 'that was a supervised learning problem.', 'start': 61.585, 'duration': 2.815}, {'text': 'And the term supervised learning [NOISE] meant', 'start': 64.4, 'duration': 4.44}, {'text': "that you were given Xs which was a picture of what's in front of the car,", 'start': 68.84, 'duration': 5.21}, {'text': 'and the algorithm [NOISE] had to map that to', 'start': 74.05, 'duration': 2.74}, {'text': 'an output Y which was the steering direction.', 'start': 76.79, 'duration': 2.955}, {'text': 'And that was a regression problem,', 'start': 79.745, 'duration': 2.565}, {'text': '[NOISE] because the output Y that you want is a continuous value, right?', 'start': 82.31, 'duration': 6.045}, {'text': 'As opposed to a classification problem where Y is the speed.', 'start': 88.355, 'duration': 2.905}, {'text': "And we'll talk about classification, um,", 'start': 91.26, 'duration': 2.04}, {'text': 'next Monday, but supervised learning regression.', 'start': 93.3, 'duration': 2.975}, {'text': 'So I think the simplest,', 'start': 96.275, 'duration': 1.815}, {'text': 'maybe the simplest possible learning algorithm,', 'start': 98.09, 'duration': 2.369}, {'text': 'a supervised learning regression problem, is linear regression.', 'start': 100.459, 'duration': 4.231}, {'text': 'And to motivate that,', 'start': 104.69, 'duration': 2.16}, {'text': 'rather than using a self-driving car example which is quite complicated,', 'start': 106.85, 'duration': 4.23}, {'text': "we'll- we'll build up a supervised learning algorithm using a simpler example.", 'start': 111.08, 'duration': 3.93}, {'text': "Um, so let's say you want to predict or estimate the prices of houses.", 'start': 115.01, 'duration': 6.09}, {'text': 'So [NOISE] the way you build a learning algorithm is', 'start': 121.1, 'duration': 3.45}, {'text': 'start by collecting a data-set of houses, and their prices.', 'start': 124.55, 'duration': 3.875}, {'text': 'Um, so this is a data-set that we collected off Craigslist a little bit back.', 'start': 128.425, 'duration': 4.465}, {'text': 'This is data from Portland, Oregon.', 'start': 132.89, 'duration': 2.96}, {'text': "[NOISE] But so there's the size of a house in square feet, [NOISE] um,", 'start': 135.85, 'duration': 4.48}, {'text': "and there's the price of a house in thousands of dollars, [NOISE] right?", 'start': 140.33, 'duration': 4.9}, {'text': "And so there's a house that is 2,104 square feet whose asking price was $400,000.", 'start': 145.23, 'duration': 7.55}, {'text': 'Um, [NOISE] house with, uh,', 'start': 152.78, 'duration': 2.025}, {'text': 'that size, with that price,', 'start': 154.805, 'duration': 2.745}, {'text': '[NOISE] and so on.', 'start': 157.55, 'duration': 7.485}, {'text': 'Okay? Um, and maybe more conventionally if you plot this data,', 'start': 165.035, 'duration': 6.455}, {'text': "there's a size, there's a price.", 'start': 171.49, 'duration': 2.42}, {'text': 'So you have some dataset like that.', 'start': 173.91, 'duration': 2.475}, {'text': "And what we'll end up doing today is fit a straight line to this data, right?", 'start': 176.385, 'duration': 3.695}, {'text': '[NOISE] And go through how to do that.', 'start': 180.08, 'duration': 1.395}, {'text': 'So in supervised learning, um,', 'start': 181.475, 'duration': 2.805}, {'text': 'the [NOISE] process of supervised learning is that you have', 'start': 184.28, 'duration': 3.0}, {'text': 'a training set such as the data-set that I drew on the left,', 'start': 187.28, 'duration': 4.255}, {'text': 'and you feed this to a learning algorithm, [NOISE] right?', 'start': 191.535, 'duration': 7.9}, {'text': 'And the job of the learning algorithm is to output a function,', 'start': 199.435, 'duration': 4.345}, {'text': 'uh, to make predictions about housing prices.', 'start': 203.78, 'duration': 2.91}, {'text': 'And by convention, um,', 'start': 206.69, 'duration': 2.13}, {'text': "I'm gonna call this a function that it outputs a hypothesis, [NOISE] right?", 'start': 208.82, 'duration': 6.815}, {'text': 'And the job of the hypothesis is, [NOISE] you know,', 'start': 215.635, 'duration': 2.83}, {'text': 'it will- it can input the size of a new house,', 'start': 218.465, 'duration': 3.645}, {'text': "or the size of a different house that you haven't seen yet,", 'start': 222.11, 'duration': 2.16}, {'text': '[NOISE] and will output the estimated [NOISE] price.', 'start': 224.27, 'duration': 6.625}, {'text': 'Okay? Um, so the job of the learning algorithm is to input a training set,', 'start': 230.895, 'duration': 3.945}, {'text': 'and output a hypothesis.', 'start': 234.84, 'duration': 1.19}, {'text': 'The job of the hypothesis is to take as input,', 'start': 236.03, 'duration': 2.61}, {'text': 'any size of a house,', 'start': 238.64, 'duration': 1.11}, {'text': 'and try to tell you what it thinks should be the price of that house.', 'start': 239.75, 'duration': 4.71}, {'text': 'Now, when designing a learning algorithm,', 'start': 244.46, 'duration': 4.109}, {'text': 'um, and- and, you know,', 'start': 248.569, 'duration': 1.096}, {'text': 'even though linear regression, right?', 'start': 249.665, 'duration': 2.175}, {'text': 'You may have seen it in a linear algebra class before,', 'start': 251.84, 'duration': 2.05}, {'text': 'or some other class before, um,', 'start': 253.89, 'duration': 1.89}, {'text': 'the way you go about structuring a machine learning algorithm is important.', 'start': 255.78, 'duration': 3.77}, {'text': 'And design choices of,', 'start': 259.55, 'duration': 1.59}, {'text': 'you know, what is the workflow?', 'start': 261.14, 'duration': 1.32}, {'text': 'What is the data-set? What is the hypothesis?', 'start': 262.46, 'duration': 1.89}, {'text': 'How does this represent the hypothesis?', 'start': 264.35, 'duration': 1.095}, {'text': 'These are the key decisions you have to make in pretty much every supervised learning,', 'start': 265.445, 'duration': 4.755}, {'text': "every machine learning algorithm's design.", 'start': 270.2, 'duration': 1.56}, {'text': 'So, uh, as we go through linear regression,', 'start': 271.76, 'duration': 2.64}, {'text': 'I will try to describe the concepts clearly as well', 'start': 274.4, 'duration': 2.64}, {'text': "because they'll lay the foundation for the rest of the algorithms.", 'start': 277.04, 'duration': 2.94}, {'text': "Sometimes it's much more complicated with the algorithms you'll see later this quarter.", 'start': 279.98, 'duration': 3.675}, {'text': 'So when designing a learning algorithm the first thing we need to ask is, um,', 'start': 283.655, 'duration': 4.275}, {'text': '[NOISE] how- how do you represent the hypothesis, H, right?', 'start': 287.93, 'duration': 7.545}, {'text': 'And in linear regression,', 'start': 295.475, 'duration': 1.68}, {'text': 'for the purpose of this lecture,', 'start': 297.155, 'duration': 1.335}, {'text': "[NOISE] we're going to say that, um,", 'start': 298.49, 'duration': 2.13}, {'text': 'the hypothesis is going to be [NOISE] that.', 'start': 300.62, 'duration': 5.73}, {'text': 'Right? That the input, uh, size X,', 'start': 306.35, 'duration': 2.935}, {'text': 'and output a number as a- as a linear function,', 'start': 309.285, 'duration': 3.945}, {'text': 'um, of the size X, okay?', 'start': 313.23, 'duration': 2.61}, {'text': 'And then, the mathematicians in the room,', 'start': 315.84, 'duration': 1.95}, {'text': "you'll say technically this is an affine function.", 'start': 317.79, 'duration': 1.82}, {'text': "It was a linear function, there's no theta 0, technically, you know,", 'start': 319.61, 'duration': 3.23}, {'text': 'but- but the machine learning sometimes just calls this a linear function,', 'start': 322.84, 'duration': 2.65}, {'text': "but technically it's an affine function. Doesn't- doesn't matter.", 'start': 325.49, 'duration': 2.955}, {'text': 'Um, so more generally in- in this example we have just one input feature X.', 'start': 328.445, 'duration': 7.485}, {'text': 'More generally, if you have multiple input features,', 'start': 335.93, 'duration': 3.7}, {'text': 'so if you have more data,', 'start': 339.63, 'duration': 1.535}, {'text': 'more information about these houses,', 'start': 341.165, 'duration': 2.46}, {'text': 'such as number of bedrooms [NOISE] Excuse me,', 'start': 343.625, 'duration': 5.935}, {'text': 'my handwriting is not big.', 'start': 349.56, 'duration': 1.26}, {'text': "That's the word bedrooms, [NOISE] right?", 'start': 350.82, 'duration': 4.725}, {'text': 'Then, I guess- [NOISE] All right.', 'start': 355.545, 'duration': 4.125}, {'text': 'Yeah. Cool. My- my- my father-in-law lives a little bit outside Portland,', 'start': 359.67, 'duration': 4.46}, {'text': "uh, and he's actually really into real estate.", 'start': 364.13, 'duration': 1.47}, {'text': 'So this is actually a real data-set from Portland.', 'start': 365.6, 'duration': 1.83}, {'text': '[LAUGHTER] Um, so more generally,', 'start': 367.43, 'duration': 3.82}, {'text': 'uh, if you know the size,', 'start': 371.25, 'duration': 1.91}, {'text': 'as well as the number of bedrooms in these houses,', 'start': 373.16, 'duration': 2.895}, {'text': 'then you may have two input features [NOISE] where X1 is the size,', 'start': 376.055, 'duration': 4.86}, {'text': 'and X2 is the number of bedrooms.', 'start': 380.915, 'duration': 3.705}, {'text': "[NOISE] Um, I'm using the pound sign bedrooms to denote number of bedrooms,", 'start': 384.62, 'duration': 5.58}, {'text': 'and you might say that you estimate the size of a house as,', 'start': 390.2, 'duration': 4.26}, {'text': 'um, h of x equals,', 'start': 394.46, 'duration': 1.26}, {'text': 'theta 0 plus theta 1,', 'start': 395.72, 'duration': 1.5}, {'text': '[NOISE] X1, plus theta 2, X2,', 'start': 397.22, 'duration': 4.285}, {'text': 'where X1 is the size of the house,', 'start': 401.505, 'duration': 2.475}, {'text': 'and X2 is- [NOISE] is the number of bedrooms.', 'start': 403.98, 'duration': 4.8}, {'text': 'Okay? Um, so in order to-', 'start': 408.78, 'duration': 3.21}, {'text': '[NOISE] so in order to simplify the notation,', 'start': 411.99, 'duration': 8.73}, {'text': '[NOISE] um, [NOISE] in order to make that notation a little bit more compact,', 'start': 420.72, 'duration': 10.375}, {'text': "um, I'm also gonna introduce this other notation where,", 'start': 431.095, 'duration': 4.2}, {'text': 'um, we want to write a hypothesis,', 'start': 435.295, 'duration': 4.075}, {'text': 'as sum from J equals 0-2 of theta JXJ,', 'start': 439.8, 'duration': 11.04}, {'text': 'so this is the summation,', 'start': 450.84, 'duration': 2.045}, {'text': 'where for conciseness we define X0 to be equal to 1, okay?', 'start': 452.885, 'duration': 6.08}, {'text': 'See we define- if we define X0 to be', 'start': 458.965, 'duration': 2.695}, {'text': 'a dummy feature that always takes on the value of 1,', 'start': 461.66, 'duration': 3.455}, {'text': 'then you can write the hypothesis h of x this way,', 'start': 465.115, 'duration': 2.995}, {'text': 'sum from J equals 0-2,', 'start': 468.11, 'duration': 1.83}, {'text': 'or just theta JXJ, okay?', 'start': 469.94, 'duration': 1.65}, {'text': "It's the same with that equation that you saw to the upper right.", 'start': 471.59, 'duration': 3.705}, {'text': 'And so here theta becomes a three-dimensional parameter,', 'start': 475.295, 'duration': 4.91}, {'text': 'theta 0, theta 1, theta 2.', 'start': 480.205, 'duration': 2.9}, {'text': 'This index starting from 0,', 'start': 483.105, 'duration': 2.08}, {'text': 'and the features become a three dimensional feature vector X0, X1,', 'start': 485.185, 'duration': 6.485}, {'text': 'X2, where X0 is always 1,', 'start': 491.67, 'duration': 2.295}, {'text': 'X1 is the size of the house,', 'start': 493.965, 'duration': 1.635}, {'text': 'and X2 is the number of bedrooms of the house, okay?', 'start': 495.6, 'duration': 4.68}, {'text': 'So, um, to introduce a bit more terminology.', 'start': 500.28, 'duration': 3.905}, {'text': 'Theta [NOISE] is called the parameters, um,', 'start': 504.185, 'duration': 8.335}, {'text': 'of the learning algorithm,', 'start': 512.52, 'duration': 1.605}, {'text': 'and the job [NOISE] of the learning algorithm is to choose parameters theta,', 'start': 514.125, 'duration': 5.085}, {'text': 'that allows you to make good predictions about your prices of houses, right?', 'start': 519.21, 'duration': 5.03}, {'text': "Um, and just to lay out some more notation that we're gonna use throughout this quarter.", 'start': 524.24, 'duration': 6.79}, {'text': "We're gonna use a standard that, uh, M,", 'start': 531.03, 'duration': 4.09}, {'text': "we'll define as the number of training examples.", 'start': 536.66, 'duration': 7.24}, {'text': 'So M is going to be the number of rows,', 'start': 543.9, 'duration': 2.76}, {'text': '[NOISE] right, in the table above, um,', 'start': 546.66, 'duration': 5.17}, {'text': 'where, you know, each house you have in your training set.', 'start': 552.2, 'duration': 3.06}, {'text': 'This one training example.', 'start': 555.26, 'duration': 1.88}, {'text': "Um, you've already seen [NOISE] me use X to denote the inputs,", 'start': 557.14, 'duration': 5.94}, {'text': 'um, and often the inputs are called  features.', 'start': 563.36, 'duration': 5.42}, {'text': "Um, you know, I think, I don't know,", 'start': 568.78, 'duration': 2.36}, {'text': 'as- as- as a- as a emerging discipline grows up, right,', 'start': 571.14, 'duration': 3.44}, {'text': 'notation kind of emerges depending on what', 'start': 574.58, 'duration': 2.31}, {'text': 'different scientists use for the first time when you write a paper.', 'start': 576.89, 'duration': 2.58}, {'text': "So I think that, I don't know,", 'start': 579.47, 'duration': 1.89}, {'text': 'I think that the fact that we call these things hypotheses,', 'start': 581.36, 'duration': 2.28}, {'text': "frankly, I don't think that's a great name.", 'start': 583.64, 'duration': 1.62}, {'text': 'But- but I think someone many decades ago wrote a few papers calling it a hypothesis,', 'start': 585.26, 'duration': 4.41}, {'text': "and then others followed, and we're kind of stuck with some of this terminology.", 'start': 589.67, 'duration': 3.24}, {'text': 'But X is called input features,', 'start': 592.91, 'duration': 1.84}, {'text': 'or sometimes input attributes, um,', 'start': 594.75, 'duration': 2.28}, {'text': 'and Y [NOISE] is the output, right?', 'start': 597.03, 'duration': 4.995}, {'text': 'And sometimes we call this the target variable.', 'start': 602.025, 'duration': 2.135}, {'text': '[NOISE] Okay.', 'start': 604.16, 'duration': 3.76}, {'text': 'Uh, so x, y is,', 'start': 607.92, 'duration': 3.79}, {'text': 'uh, one training example.', 'start': 611.71, 'duration': 1.29}, {'text': '[NOISE] Um, and, uh,', 'start': 613.0, 'duration': 8.97}, {'text': "I'm going to use this notation, um,", 'start': 621.97, 'duration': 4.005}, {'text': 'x_i,  y_i in parentheses to denote', 'start': 625.975, 'duration': 5.82}, {'text': 'the i_th training example.', 'start': 631.795, 'duration': 6.0}, {'text': "Okay. So the superscript parentheses i, that's not exponentiation.", 'start': 637.795, 'duration': 4.635}, {'text': 'Uh, I think that as suppo- uh, this is- um,', 'start': 642.43, 'duration': 3.375}, {'text': 'this notation x_i, y_ i,', 'start': 645.805, 'duration': 2.37}, {'text': 'this is just a way of,', 'start': 648.175, 'duration': 1.365}, {'text': 'uh, writing an index into the table of training examples above.', 'start': 649.54, 'duration': 3.96}, {'text': 'Okay. So, so maybe, for example,', 'start': 653.5, 'duration': 2.01}, {'text': 'if the first training example is, uh,', 'start': 655.51, 'duration': 1.695}, {'text': 'the size- the house of size 2104,', 'start': 657.205, 'duration': 2.685}, {'text': 'so x_1_1 would be equal to 2104,', 'start': 659.89, 'duration': 6.96}, {'text': 'right, because this is the size of the first house in the training example.', 'start': 666.85, 'duration': 3.465}, {'text': 'And I guess, uh, x, um,', 'start': 670.315, 'duration': 3.3}, {'text': 'second example, feature one would be 1416 with our example above.', 'start': 673.615, 'duration': 6.215}, {'text': 'So the superscript in parentheses is just some,', 'start': 679.83, 'duration': 3.13}, {'text': "uh, uh, yes, it's, it's just the, um,", 'start': 682.96, 'duration': 3.515}, {'text': 'index into the different training examples', 'start': 686.475, 'duration': 2.73}, {'text': 'where i- superscript i here would run from 1 through m,', 'start': 689.205, 'duration': 4.33}, {'text': '1 through the number of training examples you have.', 'start': 693.535, 'duration': 2.505}, {'text': 'Um, and then one last bit of notation, um,', 'start': 696.04, 'duration': 4.2}, {'text': "I'm going to use n to", 'start': 700.24, 'duration': 2.34}, {'text': 'denote the number of features you have for the supervised learning problem.', 'start': 702.58, 'duration': 6.39}, {'text': 'So in this example, uh,', 'start': 708.97, 'duration': 1.5}, {'text': 'n is equal to 2, right?', 'start': 710.47, 'duration': 2.28}, {'text': 'Because we have two features which is,', 'start': 712.75, 'duration': 2.445}, {'text': 'um, the size of house and the number of bedrooms, so two features.', 'start': 715.195, 'duration': 3.84}, {'text': 'Which is why you can take this and,', 'start': 719.035, 'duration': 4.785}, {'text': 'and write this, um,', 'start': 723.82, 'duration': 3.24}, {'text': 'as the sum from j equals 0 to n. Um, and so here,', 'start': 727.06, 'duration': 7.95}, {'text': 'x and Theta are n plus 1 dimensional because we added the extra,', 'start': 735.01, 'duration': 5.919}, {'text': 'um, x_0 and Theta_0.', 'start': 740.929, 'duration': 2.981}, {'text': 'Okay. So- so we have two features then these are three-dimensional vectors.', 'start': 743.91, 'duration': 3.94}, {'text': 'And more generally, if you have n features, uh, you,', 'start': 747.85, 'duration': 2.565}, {'text': 'you end up with x and Theta being n plus 1 dimensional features. All right.', 'start': 750.415, 'duration': 6.135}, {'text': 'And, you know, uh,', 'start': 756.55, 'duration': 1.77}, {'text': 'you see this notation in multiple times,', 'start': 758.32, 'duration': 2.16}, {'text': 'in multiple algorithms throughout this quarter.', 'start': 760.48, 'duration': 1.89}, {'text': 'So if you, you know,', 'start': 762.37, 'duration': 1.71}, {'text': "don't manage to memorize all these symbols right now, don't worry about it.", 'start': 764.08, 'duration': 2.82}, {'text': "You'll see them over and over and they become familiar. All right.", 'start': 766.9, 'duration': 4.785}, {'text': 'So, um, given the data set and given that this is the way you define the hypothesis,', 'start': 771.685, 'duration': 6.63}, {'text': 'how do you choose the parameters, right?', 'start': 778.315, 'duration': 2.325}, {'text': "So you- the learning algorithm's job is to choose values for the parameters", 'start': 780.64, 'duration': 3.33}, {'text': 'Theta so that it can output a hypothesis.', 'start': 783.97, 'duration': 3.54}, {'text': 'So how do you choose parameters Theta?', 'start': 787.51, 'duration': 2.64}, {'text': "Well, what we'll do, um,", 'start': 790.15, 'duration': 2.73}, {'text': "is let's choose Theta", 'start': 792.88, 'duration': 3.159}, {'text': 'such that h of x is close to y,', 'start': 803.1, 'duration': 6.07}, {'text': 'uh, for the training examples.', 'start': 809.17, 'duration': 3.49}, {'text': 'Okay. So, um, and I think the final bit of notation, um,', 'start': 814.32, 'duration': 6.88}, {'text': "I've been writing h of x as a function of the features of the house,", 'start': 821.2, 'duration': 4.92}, {'text': 'as a function of the size and number of bedrooms of the house.', 'start': 826.12, 'duration': 2.4}, {'text': '[NOISE] Um, sometimes we emphasize that h depends both', 'start': 828.52, 'duration': 3.45}, {'text': 'on the parameters Theta and on the input features x. Um,', 'start': 831.97, 'duration': 4.56}, {'text': "we're going to use h_Theta", 'start': 836.53, 'duration': 4.499}, {'text': 'of x to emphasize that the hypothesis depends both on the parameters and on the,', 'start': 841.029, 'duration': 5.356}, {'text': 'you know, input features x, right?', 'start': 846.385, 'duration': 1.605}, {'text': 'But, uh, sometimes for notational convenience,', 'start': 847.99, 'duration': 2.55}, {'text': 'I just write this as h of x,', 'start': 850.54, 'duration': 1.26}, {'text': 'sometimes I include the Theta there, and they mean the same thing.', 'start': 851.8, 'duration': 3.12}, {'text': "It's just, uh, maybe a abbreviation in notation.", 'start': 854.92, 'duration': 2.835}, {'text': 'Okay. But so in order to,', 'start': 857.755, 'duration': 2.985}, {'text': 'um, learn a set of parameters,', 'start': 860.74, 'duration': 2.835}, {'text': "what we'll want to do is choose a parameters Theta", 'start': 863.575, 'duration': 3.66}, {'text': 'so that at least for the houses whose prices you know, that, you know,', 'start': 867.235, 'duration': 3.705}, {'text': 'the learning algorithm outputs prices that are close to', 'start': 870.94, 'duration': 3.03}, {'text': 'what you know where the correct price is for that set of houses,', 'start': 873.97, 'duration': 3.42}, {'text': 'what the correct asking price is for those houses.', 'start': 877.39, 'duration': 3.71}, {'text': 'And so more formally, um,', 'start': 881.1, 'duration': 3.15}, {'text': 'in the linear regression algorithm,', 'start': 884.25, 'duration': 2.025}, {'text': 'also called ordinary least squares.', 'start': 886.275, 'duration': 1.635}, {'text': 'With linear regression, um,', 'start': 887.91, 'duration': 1.515}, {'text': 'we will want to minimize,', 'start': 889.425, 'duration': 5.93}, {'text': "I'm gonna build out this equation one piece at a time, okay?", 'start': 895.355, 'duration': 3.515}, {'text': 'Minimize the square difference between what the hypothesis outputs,', 'start': 898.87, 'duration': 5.31}, {'text': 'h_Theta of x minus y squared, right?', 'start': 904.18, 'duration': 10.29}, {'text': "So let's say we wanna minimize the squared difference between the prediction,", 'start': 914.47, 'duration': 3.81}, {'text': 'which is h of x and y,', 'start': 918.28, 'duration': 1.59}, {'text': 'which is the correct price.', 'start': 919.87, 'duration': 1.635}, {'text': 'Um, and so what we want to do is choose values of Theta that minimizes that.', 'start': 921.505, 'duration': 8.51}, {'text': 'Um, to fill this out, you know,', 'start': 930.015, 'duration': 1.95}, {'text': 'you have m training examples.', 'start': 931.965, 'duration': 2.22}, {'text': "So I'm going to sum from i equals 1 through m of that square difference.", 'start': 934.185, 'duration': 9.07}, {'text': 'So this is sum over i equals 1 through all,', 'start': 943.255, 'duration': 3.0}, {'text': 'say, 50 examples you have, right?', 'start': 946.255, 'duration': 2.16}, {'text': 'Um, of the square difference between what', 'start': 948.415, 'duration': 2.865}, {'text': 'your algorithm predicts and what the true price of the house is.', 'start': 951.28, 'duration': 3.21}, {'text': 'Um, and then finally, by convention,', 'start': 954.49, 'duration': 4.53}, {'text': 'we put a one-half there- put a one-half constant there because, uh,', 'start': 959.02, 'duration': 4.02}, {'text': 'when we take derivatives to minimize this later,', 'start': 963.04, 'duration': 2.025}, {'text': 'putting a one-half there would make some of the math a little bit simpler.', 'start': 965.065, 'duration': 2.625}, {'text': 'So, you know, changing one- adding a one-half.', 'start': 967.69, 'duration': 2.25}, {'text': 'Minimizing that formula should give you the same as minimizing one-half of that but we', 'start': 969.94, 'duration': 4.11}, {'text': 'often put a one-half there so to make the math a little bit simpler later, okay?', 'start': 974.05, 'duration': 4.725}, {'text': 'And so in linear regression,', 'start': 978.775, 'duration': 3.674}, {'text': "we're gonna define the cost function J of Theta to be equal to that.", 'start': 982.449, 'duration': 6.136}, {'text': "And, uh, [NOISE] we'll find parameters", 'start': 988.585, 'duration': 4.785}, {'text': 'Theta that minimizes the cost function J of Theta, okay?', 'start': 993.37, 'duration': 5.895}, {'text': "Um, and, the questions I've often gotten is,", 'start': 999.265, 'duration': 4.23}, {'text': 'you know, why squared error?', 'start': 1003.495, 'duration': 1.185}, {'text': 'Why not absolute error, or this error to the power of 4?', 'start': 1004.68, 'duration': 3.165}, {'text': "We'll talk more about that when we talk about, um, uh,", 'start': 1007.845, 'duration': 3.78}, {'text': 'when, when, when we talk about the generalization of, uh, linear regression.', 'start': 1011.625, 'duration': 4.86}, {'text': 'Um, when we talk about generalized linear models,', 'start': 1016.485, 'duration': 1.905}, {'text': "which we'll do next week, you'll see that, um,", 'start': 1018.39, 'duration': 2.43}, {'text': 'uh, linear regression is a special case', 'start': 1020.82, 'duration': 3.0}, {'text': 'of a bigger family of algorithms called generalizing your models.', 'start': 1023.82, 'duration': 3.135}, {'text': 'And that, uh, using squared error corresponds to a Gaussian, but the- we, we,', 'start': 1026.955, 'duration': 5.025}, {'text': "we'll justify maybe a little bit more why squared error", 'start': 1031.98, 'duration': 3.66}, {'text': 'rather than absolute error or errors to the power of 4, uh, next week.', 'start': 1035.64, 'duration': 4.78}, {'text': 'So, um, let me just check,', 'start': 1040.49, 'duration': 2.77}, {'text': 'see if any questions,', 'start': 1043.26, 'duration': 1.62}, {'text': '[NOISE] at this point. No, okay. Cool.', 'start': 1044.88, 'duration': 7.3}, {'text': 'All right. So, um,', 'start': 1061.58, 'duration': 10.975}, {'text': "so let's- next let's see how you can implement", 'start': 1072.555, 'duration': 4.335}, {'text': 'an algorithm to find the value of Theta that minimizes J of Theta.', 'start': 1076.89, 'duration': 4.32}, {'text': 'That- that minimizes the cost function J of Theta.', 'start': 1081.21, 'duration': 3.765}, {'text': "Um, we're going to use an algorithm called gradient descent.", 'start': 1084.975, 'duration': 3.985}, {'text': 'And, um, you know,', 'start': 1092.42, 'duration': 3.46}, {'text': 'this is my first time teaching in this classroom,', 'start': 1095.88, 'duration': 1.62}, {'text': 'so trying to figure out logistics like this.', 'start': 1097.5, 'duration': 2.52}, {'text': "All right. Let's get rid of the chair.", 'start': 1100.02, 'duration': 2.26}, {'text': 'Cool, um, all right.', 'start': 1102.28, 'duration': 5.27}, {'text': 'And so with, uh,', 'start': 1107.55, 'duration': 1.155}, {'text': 'gradient descent we are going to start with some value of Theta,', 'start': 1108.705, 'duration': 8.94}, {'text': 'um, and it could be, you know,', 'start': 1117.645, 'duration': 3.165}, {'text': 'Theta equals the vector of all zeros would be a reasonable default.', 'start': 1120.81, 'duration': 3.39}, {'text': "We can initialize it randomly, the count doesn't really matter.", 'start': 1124.2, 'duration': 2.565}, {'text': 'But, uh, Theta is this three-dimensional vector.', 'start': 1126.765, 'duration': 2.985}, {'text': "And I'm writing 0 with an arrow on top to denote the vector of all 0s.", 'start': 1129.75, 'duration': 5.31}, {'text': "So 0 with an arrow on top that's a vector that says 0,", 'start': 1135.06, 'duration': 2.46}, {'text': '0, 0, everywhere, right.', 'start': 1137.52, 'duration': 1.455}, {'text': 'So, um, uh, so sought to some, you know,', 'start': 1138.975, 'duration': 3.13}, {'text': "initial value of Theta and we're going to keep changing Theta,", 'start': 1142.105, 'duration': 10.42}, {'text': 'um, to reduce J of Theta, okay?', 'start': 1152.525, 'duration': 7.9}, {'text': 'So let me show you a,', 'start': 1160.425, 'duration': 3.81}, {'text': 'um- vi- vis- let me show you a visualization of gradient descent,', 'start': 1164.235, 'duration': 7.875}, {'text': "and then- and then we'll write out the math.", 'start': 1172.11, 'duration': 1.95}, {'text': '[NOISE] Um, so- all right.', 'start': 1174.06, 'duration': 6.555}, {'text': "Let's say you want to minimize some function J of Theta and,", 'start': 1180.615, 'duration': 4.14}, {'text': "uh, it's important to get the axes right in this diagram, right?", 'start': 1184.755, 'duration': 3.075}, {'text': 'So in this diagram the horizontal axes are Theta 0 and Theta 1.', 'start': 1187.83, 'duration': 5.085}, {'text': 'And what you want to do is find values for Theta 0 and Theta 1.', 'start': 1192.915, 'duration': 5.06}, {'text': "In our- I- I- In our example it's actually Theta 0, Theta 1,", 'start': 1197.975, 'duration': 2.725}, {'text': "Theta 2 because Theta's 3-dimensional but I can't plot that.", 'start': 1200.7, 'duration': 2.58}, {'text': "So I'm just using Theta 0 and Theta 1.", 'start': 1203.28, 'duration': 2.13}, {'text': 'But what you want to do is find values for Theta 0 and Theta 1, right?', 'start': 1205.41, 'duration': 5.145}, {'text': "That's the, um, uh, right,", 'start': 1210.555, 'duration': 6.045}, {'text': 'you wanna find values of Theta 0 and Theta', 'start': 1216.6, 'duration': 2.97}, {'text': '1 that minimizes the height of the surface j of Theta.', 'start': 1219.57, 'duration': 3.21}, {'text': 'So maybe this- this looks like a good- pretty good point or something, okay?', 'start': 1222.78, 'duration': 3.3}, {'text': 'Um, and so in gradient descent you, you know,', 'start': 1226.08, 'duration': 3.84}, {'text': 'start off at some point on this surface and you do that by initializing', 'start': 1229.92, 'duration': 4.965}, {'text': 'Theta 0 and Theta 1 either randomly or to the value', 'start': 1234.885, 'duration': 2.715}, {'text': "of all zeros or something doesn't- doesn't matter too much.", 'start': 1237.6, 'duration': 2.88}, {'text': 'And, um, what you do is, uh,', 'start': 1240.48, 'duration': 2.145}, {'text': 'im- imagine that you are standing on this lower hill, right', 'start': 1242.625, 'duration': 2.895}, {'text': 'standing at the point of that little x or that little cross.', 'start': 1245.52, 'duration': 3.63}, {'text': 'Um, what you do in gradient descent is, uh,', 'start': 1249.15, 'duration': 2.7}, {'text': 'turn on- turn around all 360 degrees and look all around', 'start': 1251.85, 'duration': 3.69}, {'text': 'you and see if you were to take a tiny little step, you know,', 'start': 1255.54, 'duration': 3.69}, {'text': 'take a tiny little baby step,', 'start': 1259.23, 'duration': 1.56}, {'text': 'in what direction should you take a little step to go downhill as fast', 'start': 1260.79, 'duration': 5.01}, {'text': "as possible because you're trying to go downhill which", 'start': 1265.8, 'duration': 2.43}, {'text': 'is- goes to the lowest possible elevation,', 'start': 1268.23, 'duration': 2.52}, {'text': 'goes to the lowest possible point of J of Theta, okay?', 'start': 1270.75, 'duration': 3.69}, {'text': 'So what gradient descent will do is, uh,', 'start': 1274.44, 'duration': 1.845}, {'text': 'stand at that point look around,', 'start': 1276.285, 'duration': 1.92}, {'text': 'look all- all around you and say, well,', 'start': 1278.205, 'duration': 2.355}, {'text': 'what direction should I take a little step in to go downhill as', 'start': 1280.56, 'duration': 3.21}, {'text': 'quickly as possible because you want to minimize, uh, J of Theta.', 'start': 1283.77, 'duration': 3.51}, {'text': 'You wanna minim- reduce the value of J of Theta,', 'start': 1287.28, 'duration': 2.67}, {'text': 'you know, go to the lowest possible elevation on this hill.', 'start': 1289.95, 'duration': 2.91}, {'text': 'Um, and so gradient descent will take that little baby step, right?', 'start': 1292.86, 'duration': 5.58}, {'text': 'And then- and then repeat.', 'start': 1298.44, 'duration': 1.65}, {'text': "Uh, now you're a little bit lower on the surface.", 'start': 1300.09, 'duration': 3.225}, {'text': 'So you again take a look all around you and say oh it looks like that hill,', 'start': 1303.315, 'duration': 3.33}, {'text': 'that- that little direction is the steepest direction or the steepest gradient downhill.', 'start': 1306.645, 'duration': 4.83}, {'text': 'So you take another little step,', 'start': 1311.475, 'duration': 1.635}, {'text': 'take another step- another step and so on,', 'start': 1313.11, 'duration': 4.035}, {'text': 'until, um, uh, until you- until you get to a hopefully a local optimum.', 'start': 1317.145, 'duration': 6.33}, {'text': 'Now one property of gradient descent is that, um, uh,', 'start': 1323.475, 'duration': 4.035}, {'text': 'depend on where you initialize parameters,', 'start': 1327.51, 'duration': 1.755}, {'text': 'you can get to local diff- different points, right?', 'start': 1329.265, 'duration': 2.745}, {'text': 'So previously, you had started it at that lower point x.', 'start': 1332.01, 'duration': 3.105}, {'text': 'But imagine if, uh,', 'start': 1335.115, 'duration': 1.35}, {'text': 'you had started it, you know,', 'start': 1336.465, 'duration': 1.185}, {'text': 'just a few steps over to the right, right?', 'start': 1337.65, 'duration': 2.625}, {'text': 'At that- at that new x instead of the one on the left.', 'start': 1340.275, 'duration': 3.06}, {'text': 'If you had run gradient descent from that new point then, uh,', 'start': 1343.335, 'duration': 3.255}, {'text': 'that would have been the first step,', 'start': 1346.59, 'duration': 1.515}, {'text': 'that would be the second step and so on.', 'start': 1348.105, 'duration': 2.445}, {'text': 'And you would have gotten to', 'start': 1350.55, 'duration': 1.41}, {'text': 'a different local optimum- to a different local minima, okay?', 'start': 1351.96, 'duration': 4.68}, {'text': 'Um, it turns out that when you run gradient descents on linear regression,', 'start': 1356.64, 'duration': 5.025}, {'text': 'it turns out that, uh, uh, uh,', 'start': 1361.665, 'duration': 2.55}, {'text': "there will not be local optimum but we'll talk about that in a little bit, okay?", 'start': 1364.215, 'duration': 4.965}, {'text': "So let's formalize the [NOISE] gradient descent algorithm.", 'start': 1369.18, 'duration': 6.7}, {'text': 'In gradient descent, um,', 'start': 1383.45, 'duration': 7.18}, {'text': 'each step of gradient descent,', 'start': 1390.63, 'duration': 1.935}, {'text': 'uh, is implemented as follows.', 'start': 1392.565, 'duration': 2.55}, {'text': 'So- so remember, in- in this example,', 'start': 1395.115, 'duration': 2.46}, {'text': 'the training set is fixed, right?', 'start': 1397.575, 'duration': 2.175}, {'text': "You- You know you've collected the data set of housing prices from Portland,", 'start': 1399.75, 'duration': 3.27}, {'text': 'Oregon so you just have that stored in your computer memory.', 'start': 1403.02, 'duration': 2.745}, {'text': 'And so the data set is fixed.', 'start': 1405.765, 'duration': 2.115}, {'text': "The cost function J is a fixed function there's function of parameters Theta,", 'start': 1407.88, 'duration': 4.08}, {'text': "and the only thing you're gonna do is tweak or modify the parameters Theta.", 'start': 1411.96, 'duration': 5.145}, {'text': 'One step of gradient descent,', 'start': 1417.105, 'duration': 2.07}, {'text': 'um, can be implemented as follows,', 'start': 1419.175, 'duration': 3.255}, {'text': 'which is Theta j gets updated as Theta j minus,', 'start': 1422.43, 'duration': 6.045}, {'text': "I'll just write this out, okay?", 'start': 1428.475, 'duration': 6.195}, {'text': 'Um, so bit more notation,', 'start': 1434.67, 'duration': 2.67}, {'text': "I'm gonna use colon equals,", 'start': 1437.34, 'duration': 2.325}, {'text': "I'm gonna use this notation to denote assignment.", 'start': 1439.665, 'duration': 3.3}, {'text': 'So what this means is,', 'start': 1442.965, 'duration': 1.095}, {'text': "we're gonna take the value on the right and assign it to Theta on the left, right?", 'start': 1444.06, 'duration': 4.215}, {'text': 'And so, um, so in other words,', 'start': 1448.275, 'duration': 2.115}, {'text': "in the notation we'll use this quarter, you know,", 'start': 1450.39, 'duration': 2.505}, {'text': 'a colon equals a plus 1.', 'start': 1452.895, 'duration': 2.565}, {'text': 'This means increment the value of a by 1.', 'start': 1455.46, 'duration': 3.0}, {'text': 'Um, whereas, you know, a equals b,', 'start': 1458.46, 'duration': 3.375}, {'text': "if I write a equals b I'm asserting a statement of fact, right?", 'start': 1461.835, 'duration': 3.825}, {'text': "I'm asserting that the value of a is equal to the value of b. Um, and hopefully,", 'start': 1465.66, 'duration': 4.89}, {'text': "I won't ever write a equals a plus 1,", 'start': 1470.55, 'duration': 3.0}, {'text': 'right because- cos that is rarely true, okay?', 'start': 1473.55, 'duration': 3.585}, {'text': 'Um, all right. So, uh,', 'start': 1477.135, 'duration': 4.635}, {'text': 'in each step of gradient descent,', 'start': 1481.77, 'duration': 2.205}, {'text': "you're going to- for each value of j,", 'start': 1483.975, 'duration': 2.61}, {'text': "so you're gonna do this for j equals 0, 1 ,2 or 0, 1,", 'start': 1486.585, 'duration': 5.655}, {'text': 'up to n, where n is the number of features.', 'start': 1492.24, 'duration': 2.775}, {'text': 'For each value of j takes either j and update it according to Theta j minus Alpha.', 'start': 1495.015, 'duration': 6.36}, {'text': 'Um, which is called the learning rate.', 'start': 1501.375, 'duration': 7.305}, {'text': 'Um, Alpha the learning rate times this formula.', 'start': 1508.68, 'duration': 4.755}, {'text': 'And this formula is the partial derivative of the cost function J', 'start': 1513.435, 'duration': 3.375}, {'text': 'of Theta with respect to the parameter,', 'start': 1516.81, 'duration': 3.69}, {'text': 'um, Theta j, okay?', 'start': 1520.5, 'duration': 1.68}, {'text': 'In- and this partial derivative notation.', 'start': 1522.18, 'duration': 1.98}, {'text': 'Uh, for those of you that, um,', 'start': 1524.16, 'duration': 2.79}, {'text': "haven't seen calculus for a while or haven't seen,", 'start': 1526.95, 'duration': 2.7}, {'text': 'you know, some of their prerequisites for a while.', 'start': 1529.65, 'duration': 1.56}, {'text': "We'll- we'll- we'll go over some more of", 'start': 1531.21, 'duration': 2.28}, {'text': 'this in a little bit greater detail in discussion section,', 'start': 1533.49, 'duration': 2.745}, {'text': "but I'll- I'll- I'll do this, um, quickly now.", 'start': 1536.235, 'duration': 4.405}, {'text': "But, um, I don't know. If, if you've taken a calculus class a while back,", 'start': 1546.74, 'duration': 3.67}, {'text': 'you may remember that the derivative of a function is,', 'start': 1550.41, 'duration': 3.495}, {'text': 'you know, defines the direction of steepest descent.', 'start': 1553.905, 'duration': 2.34}, {'text': 'So it defines the direction that allows you to go downhill as steeply as possible,', 'start': 1556.245, 'duration': 4.515}, {'text': "uh, on the, on the hill like that. There's a question.", 'start': 1560.76, 'duration': 3.51}, {'text': 'How do you determine the learning rate?', 'start': 1564.27, 'duration': 1.95}, {'text': "How do you determine the learning rate? Ah, let me get back to that. It's a good question.", 'start': 1566.22, 'duration': 2.955}, {'text': 'Uh, for now, um,', 'start': 1569.175, 'duration': 1.425}, {'text': "uh, you know, there's a theory and there's a practice.", 'start': 1570.6, 'duration': 2.76}, {'text': 'Uh, in practice, you set to 0.01.', 'start': 1573.36, 'duration': 2.4}, {'text': '[LAUGHTER].', 'start': 1575.76, 'duration': 3.48}, {'text': '[LAUGHTER] Let me say a bit more about that later.', 'start': 1579.24, 'duration': 1.41}, {'text': '[NOISE].', 'start': 1580.65, 'duration': 2.99}, {'text': 'Uh, if- if you actually- if- if you scale all the features between 0 and 1,', 'start': 1583.64, 'duration': 3.265}, {'text': 'you know, minus 1 and plus 1 or something like that, then, then, yeah.', 'start': 1586.905, 'duration': 2.97}, {'text': 'Then, then try- you can try', 'start': 1589.875, 'duration': 1.905}, {'text': 'a few values and see what lets you minimize the function best,', 'start': 1591.78, 'duration': 2.34}, {'text': 'but if the feature is scaled to plus minus 1,', 'start': 1594.12, 'duration': 3.225}, {'text': 'I usually start with 0.01 and then,', 'start': 1597.345, 'duration': 1.935}, {'text': 'and then try increasing and decreasing it.', 'start': 1599.28, 'duration': 1.62}, {'text': 'Say, say a little bit more about that.', 'start': 1600.9, 'duration': 1.02}, {'text': '[NOISE] Um, uh, all right, cool.', 'start': 1601.92, 'duration': 6.09}, {'text': "So, um, let's see.", 'start': 1608.01, 'duration': 3.69}, {'text': 'Let me just quickly [NOISE] show how the derivative calculation is done.', 'start': 1611.7, 'duration': 6.945}, {'text': "Um, and you know, I'm,", 'start': 1618.645, 'duration': 1.635}, {'text': "I'm gonna do a few more equations in this lecture,", 'start': 1620.28, 'duration': 2.25}, {'text': 'uh, and then, and then over time I think.', 'start': 1622.53, 'duration': 2.46}, {'text': 'Um, all, all of these,', 'start': 1624.99, 'duration': 1.485}, {'text': 'all of these definitions and derivations are', 'start': 1626.475, 'duration': 2.265}, {'text': 'written out in full detail in the lecture notes,', 'start': 1628.74, 'duration': 2.94}, {'text': 'uh, posted on the course website.', 'start': 1631.68, 'duration': 1.5}, {'text': "So sometimes, I'll do more math in class when, um,", 'start': 1633.18, 'duration': 3.06}, {'text': 'we want you to see the steps of the derivation and sometimes to save time in class,', 'start': 1636.24, 'duration': 3.945}, {'text': "we'll gloss over the mathematical details and leave you to read over,", 'start': 1640.185, 'duration': 3.285}, {'text': 'the full details in the lecture notes on the CS229', 'start': 1643.47, 'duration': 2.19}, {'text': 'you know, course website.', 'start': 1645.66, 'duration': 1.89}, {'text': 'Um, so partial derivative with respect to J of Theta,', 'start': 1647.55, 'duration': 4.02}, {'text': "that's the partial derivative with respect to that", 'start': 1651.57, 'duration': 3.555}, {'text': 'of one-half H of Theta of X minus Y squared.', 'start': 1655.125, 'duration': 6.96}, {'text': "Uh, and so I'm going to do", 'start': 1662.085, 'duration': 1.695}, {'text': 'a slightly simpler version assuming we have just one training example, right?', 'start': 1663.78, 'duration': 4.2}, {'text': 'The, the actual deriva- definition of J of Theta has', 'start': 1667.98, 'duration': 3.36}, {'text': 'a sum over I from 1 to M over all the training examples.', 'start': 1671.34, 'duration': 4.44}, {'text': "So I'm just forgetting that sum for now.", 'start': 1675.78, 'duration': 2.28}, {'text': 'So if you have only one training example.', 'start': 1678.06, 'duration': 2.13}, {'text': 'Um, and so from calculus,', 'start': 1680.19, 'duration': 3.24}, {'text': 'if you take the derivative of a square, you know,', 'start': 1683.43, 'duration': 2.1}, {'text': 'the 2 comes down and so that cancels out with the half.', 'start': 1685.53, 'duration': 2.79}, {'text': 'So 2 times 1.5 times, um,', 'start': 1688.32, 'duration': 2.97}, {'text': 'uh, the thing inside, right?', 'start': 1691.29, 'duration': 4.355}, {'text': 'Uh, and then by the, uh,', 'start': 1695.645, 'duration': 1.81}, {'text': 'chain rule of, uh, derivatives.', 'start': 1697.455, 'duration': 2.235}, {'text': "Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right?", 'start': 1699.69, 'duration': 7.185}, {'text': 'So if you take the derivative of a square,', 'start': 1706.875, 'duration': 2.13}, {'text': 'the two comes down and then you take', 'start': 1709.005, 'duration': 2.085}, {'text': "the derivative of what's inside and multiply that, right?", 'start': 1711.09, 'duration': 2.76}, {'text': '[NOISE] Um, and so the two and one-half cancel out.', 'start': 1713.85, 'duration': 5.985}, {'text': 'So this leaves you with H minus Y times partial derivative respect to Theta J of', 'start': 1719.835, 'duration': 7.785}, {'text': 'Theta 0X0 plus Theta 1X1', 'start': 1727.62, 'duration': 4.02}, {'text': 'plus th- th- that plus Theta NXN minus Y, right?', 'start': 1731.64, 'duration': 7.26}, {'text': 'Where I just took the definition of H of X and expanded it out to that, um, sum, right?', 'start': 1738.9, 'duration': 7.65}, {'text': 'Because, uh, H of X is just equal to that.', 'start': 1746.55, 'duration': 3.0}, {'text': 'So if you look at the partial derivative of each of these terms with respect to Theta J,', 'start': 1749.55, 'duration': 6.255}, {'text': 'the partial derivative of every one of these terms with respect to', 'start': 1755.805, 'duration': 3.825}, {'text': 'Theta J is going to z- be 0 except for,', 'start': 1759.63, 'duration': 4.125}, {'text': 'uh, the term corresponding to J, right?', 'start': 1763.755, 'duration': 2.67}, {'text': 'Because, uh, if J was equal to 1, say, right?', 'start': 1766.425, 'duration': 5.61}, {'text': "Then this term doesn't depend on Theta 1.", 'start': 1772.035, 'duration': 3.165}, {'text': 'Uh, this term, this term,', 'start': 1775.2, 'duration': 1.305}, {'text': 'all of them do not even depend on Theta 1.', 'start': 1776.505, 'duration': 2.34}, {'text': 'The only term that depends on Theta 1 is this term over there.', 'start': 1778.845, 'duration': 4.065}, {'text': 'Um, and the partial derivative of this term with respect to Theta 1 will be just X1, right?', 'start': 1782.91, 'duration': 7.365}, {'text': 'And so, um, when you take the partial derivative of', 'start': 1790.275, 'duration': 3.285}, {'text': 'this big sum with respect to say the J, uh,', 'start': 1793.56, 'duration': 3.195}, {'text': 'in- in- in- instead of just J equals 1 and with respect to Theta J in general,', 'start': 1796.755, 'duration': 4.335}, {'text': 'then the only term that even depends on Theta J is the term Theta JXJ.', 'start': 1801.09, 'duration': 7.015}, {'text': 'And so the partial derivative of all the other terms end up being 0 and', 'start': 1808.105, 'duration': 5.045}, {'text': 'partial derivative of this term with respect to Theta J is equal to XJ, okay?', 'start': 1813.15, 'duration': 6.315}, {'text': 'And so this ends up being H Theta X minus Y times XJ, okay?', 'start': 1819.465, 'duration': 8.97}, {'text': "Um, and again, listen, if you haven't,", 'start': 1828.435, 'duration': 2.295}, {'text': "if you haven't played with calculus for awhile, if you- you know,", 'start': 1830.73, 'duration': 2.28}, {'text': "don't quite remember what a partial derivative is,", 'start': 1833.01, 'duration': 1.74}, {'text': "or don't quite get what we just said. Don't worry too much about it.", 'start': 1834.75, 'duration': 2.625}, {'text': "We'll go over a bit more in the section and we- and,", 'start': 1837.375, 'duration': 2.145}, {'text': 'and then also read through the lecture notes which kind of goes over this in,', 'start': 1839.52, 'duration': 3.555}, {'text': 'in, in, um, in more detail and more slowly than,', 'start': 1843.075, 'duration': 3.03}, {'text': 'than, uh, we might do in class, okay?', 'start': 1846.105, 'duration': 3.255}, {'text': '[NOISE]', 'start': 1849.36, 'duration': 12.39}, {'text': "So, um, so plugging this- let's see.", 'start': 1861.75, 'duration': 4.5}, {'text': "So we've just calculated that this partial derivative,", 'start': 1866.25, 'duration': 4.065}, {'text': 'right, is equal to this,', 'start': 1870.315, 'duration': 2.355}, {'text': 'and so plugging it back into that formula,', 'start': 1872.67, 'duration': 2.669}, {'text': 'one step of gradient descent is,', 'start': 1875.339, 'duration': 2.071}, {'text': 'um, is the following,', 'start': 1877.41, 'duration': 2.775}, {'text': 'which is that we will- that Theta J be updated according to Theta J minus the learning', 'start': 1880.185, 'duration': 9.195}, {'text': 'rate times H of X minus', 'start': 1889.38, 'duration': 4.71}, {'text': 'Y times XJ, okay?', 'start': 1894.09, 'duration': 5.955}, {'text': "Now, I'm, I'm gonna just add a few more things in this equation.", 'start': 1900.045, 'duration': 3.75}, {'text': 'Um, so I did this with one training example, but, uh,', 'start': 1903.795, 'duration': 3.12}, {'text': 'this was- I kind of used definition of the cost function J of', 'start': 1906.915, 'duration': 2.805}, {'text': 'Theta defined using just one single training example,', 'start': 1909.72, 'duration': 4.02}, {'text': 'but you actually have M training examples.', 'start': 1913.74, 'duration': 2.535}, {'text': 'And so, um, the,', 'start': 1916.275, 'duration': 1.59}, {'text': 'the correct formula for the derivative is actually if you', 'start': 1917.865, 'duration': 3.795}, {'text': 'take this thing and sum it over all M training examples,', 'start': 1921.66, 'duration': 4.05}, {'text': 'um, the derivative of- you know,', 'start': 1925.71, 'duration': 1.41}, {'text': 'the derivative of a sum is the sum of the derivatives, right?', 'start': 1927.12, 'duration': 2.61}, {'text': 'So, um, so you actually- If, if,', 'start': 1929.73, 'duration': 3.045}, {'text': 'if you redo this derivation, you know,', 'start': 1932.775, 'duration': 2.505}, {'text': 'summing with the correct definition of J of', 'start': 1935.28, 'duration': 1.77}, {'text': 'Theta which sums over all M training examples.', 'start': 1937.05, 'duration': 2.31}, {'text': 'If you just redo that little derivation,', 'start': 1939.36, 'duration': 2.25}, {'text': 'you end up with, uh,', 'start': 1941.61, 'duration': 1.185}, {'text': 'sum equals I through M of that, right?', 'start': 1942.795, 'duration': 7.47}, {'text': 'Where remember XI is the Ith training examples input features,', 'start': 1950.265, 'duration': 4.245}, {'text': 'YI is the target label, is the, uh,', 'start': 1954.51, 'duration': 2.745}, {'text': 'price in the Ith training example, okay?', 'start': 1957.255, 'duration': 3.465}, {'text': 'Um, and so this is the actual correct formula for the partial derivative with', 'start': 1960.72, 'duration': 6.105}, {'text': "respect to that of the cost function J of Theta when it's defined using,", 'start': 1966.825, 'duration': 6.555}, {'text': 'um, uh, all of the,', 'start': 1973.38, 'duration': 2.94}, {'text': "um, [NOISE] uh, on- when it's defined using all of the training examples, okay?", 'start': 1976.32, 'duration': 4.365}, {'text': 'And so the gradient descent algorithm is to- [NOISE]', 'start': 1980.685, 'duration': 8.535}, {'text': 'Repeat until convergence, carry out this update,', 'start': 1991.45, 'duration': 5.23}, {'text': 'and in each iteration of gradient descent, uh,', 'start': 1996.68, 'duration': 3.51}, {'text': 'you do this update for j equals,', 'start': 2000.19, 'duration': 3.78}, {'text': 'uh, 0, 1 up to n. Uh,', 'start': 2003.97, 'duration': 6.09}, {'text': 'where n is the number of features.', 'start': 2010.06, 'duration': 1.71}, {'text': 'So n was 2 in our example.', 'start': 2011.77, 'duration': 2.76}, {'text': 'Okay. Um, and if you do this then,', 'start': 2014.53, 'duration': 3.705}, {'text': 'uh, uh, you know, actually let me see.', 'start': 2018.235, 'duration': 2.76}, {'text': 'Then what will happen is,', 'start': 2020.995, 'duration': 2.505}, {'text': "um, [NOISE] well, I'll show you the animation.", 'start': 2023.5, 'duration': 3.26}, {'text': 'As you fit- hopefully,', 'start': 2026.76, 'duration': 1.98}, {'text': 'you find a pretty good value of the parameters Theta.', 'start': 2028.74, 'duration': 3.795}, {'text': 'Okay. So, um, it turns out that when', 'start': 2032.535, 'duration': 5.245}, {'text': 'you plot the cost function j of Theta for a linear regression model,', 'start': 2037.78, 'duration': 5.46}, {'text': 'um, it turns out that,', 'start': 2043.24, 'duration': 2.64}, {'text': 'unlike the earlier diagram I had shown which has local optima,', 'start': 2045.88, 'duration': 4.47}, {'text': 'it turns out that if j of Theta is defined the way that, you know,', 'start': 2050.35, 'duration': 4.23}, {'text': 'we just defined it for linear regression,', 'start': 2054.58, 'duration': 1.65}, {'text': 'is the sum of squared terms, um,', 'start': 2056.23, 'duration': 2.265}, {'text': 'then j of Theta turns out to be a quadratic function, right?', 'start': 2058.495, 'duration': 2.915}, {'text': "It's a sum of these squares of terms, and so,", 'start': 2061.41, 'duration': 2.47}, {'text': 'j of Theta will always look like,', 'start': 2063.88, 'duration': 2.13}, {'text': 'look like a big bowl like this.', 'start': 2066.01, 'duration': 1.935}, {'text': 'Okay. Um, another way to look at this, uh, uh,', 'start': 2067.945, 'duration': 2.865}, {'text': 'and so and so j of Theta does not have local optima,', 'start': 2070.81, 'duration': 3.39}, {'text': 'um, or the only local optima is also the global optimum.', 'start': 2074.2, 'duration': 3.48}, {'text': 'The other way to look at the function like this is', 'start': 2077.68, 'duration': 2.16}, {'text': 'to look at the contours of this plot, right?', 'start': 2079.84, 'duration': 2.7}, {'text': 'So you plot the contours by looking at the big bowl', 'start': 2082.54, 'duration': 2.715}, {'text': 'and taking horizontal slices and plotting where the,', 'start': 2085.255, 'duration': 3.06}, {'text': 'where the curves where, where the edges of the horizontal slice is.', 'start': 2088.315, 'duration': 3.315}, {'text': 'So the contours of a big bowl or I guess a formal is,', 'start': 2091.63, 'duration': 4.14}, {'text': 'uh, of a bigger,', 'start': 2095.77, 'duration': 1.155}, {'text': 'uh, of of this quadratic function will be ellipsis,', 'start': 2096.925, 'duration': 4.95}, {'text': 'um, like these or these ovals or these ellipses like this.', 'start': 2101.875, 'duration': 3.525}, {'text': 'And so if you run gradient descent on this algorithm, um,', 'start': 2105.4, 'duration': 4.89}, {'text': "let's say I initialize, uh,", 'start': 2110.29, 'duration': 2.085}, {'text': 'my parameters at that little x,', 'start': 2112.375, 'duration': 2.385}, {'text': 'uh, shown over here, right.', 'start': 2114.76, 'duration': 3.3}, {'text': 'And usually you initialize Theta degree with a 0,', 'start': 2118.06, 'duration': 2.31}, {'text': "but but, you know, but it doesn't matter too much.", 'start': 2120.37, 'duration': 1.95}, {'text': "So let's reinitialize over there.", 'start': 2122.32, 'duration': 2.055}, {'text': 'Then, um, with one step of gradient descent,', 'start': 2124.375, 'duration': 3.105}, {'text': 'the algorithm will take that step downhill,', 'start': 2127.48, 'duration': 2.76}, {'text': 'uh, and then with a second step,', 'start': 2130.24, 'duration': 2.46}, {'text': "it'll take that step downhill whereby we, fun fact, uh,", 'start': 2132.7, 'duration': 4.5}, {'text': 'if you- if you think about the contours of the function,', 'start': 2137.2, 'duration': 2.16}, {'text': 'it turns out that the direction of steepest descent is always at 90 degrees,', 'start': 2139.36, 'duration': 3.375}, {'text': 'is always orthogonal, uh,', 'start': 2142.735, 'duration': 1.395}, {'text': 'to the contour direction, right.', 'start': 2144.13, 'duration': 1.65}, {'text': "So, I don't know, yeah.", 'start': 2145.78, 'duration': 1.935}, {'text': "I seem to remember that from my high-school something, I think it's true. All right.", 'start': 2147.715, 'duration': 4.8}, {'text': 'And so as you,', 'start': 2152.515, 'duration': 1.515}, {'text': 'as you take steps downhill, uh, uh,', 'start': 2154.03, 'duration': 2.355}, {'text': "because there's only one global minimum,", 'start': 2156.385, 'duration': 3.09}, {'text': 'um, this algorithm will eventually converge to the global minimum.', 'start': 2159.475, 'duration': 4.335}, {'text': 'Okay. And so the question just now about the choice of the learning rate Alpha.', 'start': 2163.81, 'duration': 4.725}, {'text': 'Um, if you set Alpha to be very very large,', 'start': 2168.535, 'duration': 3.135}, {'text': 'to be too large then they can overshoot, right.', 'start': 2171.67, 'duration': 2.76}, {'text': 'The steps you take can be too large and you can run past the minimum.', 'start': 2174.43, 'duration': 4.62}, {'text': 'Uh, if you set to be too small,', 'start': 2179.05, 'duration': 1.59}, {'text': 'then you need a lot of iterations and the algorithm will be slow.', 'start': 2180.64, 'duration': 3.165}, {'text': 'And so what happens in practice is, uh,', 'start': 2183.805, 'duration': 2.91}, {'text': 'usually you try a few values and and and see', 'start': 2186.715, 'duration': 3.315}, {'text': 'what value of the learning rate allows you to most efficiently,', 'start': 2190.03, 'duration': 3.495}, {'text': 'you know, drive down the value of j of Theta.', 'start': 2193.525, 'duration': 2.91}, {'text': 'Um, and if you see j of Theta increasing rather than decreasing,', 'start': 2196.435, 'duration': 3.705}, {'text': 'you see the cost function increasing rather than decreasing, then,', 'start': 2200.14, 'duration': 3.9}, {'text': "there's a very strong sign that the learning rate is,", 'start': 2204.04, 'duration': 2.55}, {'text': 'uh, too large, and so, um.', 'start': 2206.59, 'duration': 2.16}, {'text': '[NOISE] Actually what what I often do is actually try out multiple values of,', 'start': 2208.75, 'duration': 4.935}, {'text': 'um, the learning rate Alpha, and, uh,', 'start': 2213.685, 'duration': 2.775}, {'text': 'uh, and and usually try them on an exponential scale.', 'start': 2216.46, 'duration': 2.67}, {'text': 'So you try open O1,', 'start': 2219.13, 'duration': 1.785}, {'text': 'open O2, open O4,', 'start': 2220.915, 'duration': 1.455}, {'text': 'open O8 kinda like a doubling scale or some- uh, uh,', 'start': 2222.37, 'duration': 3.6}, {'text': 'or doubling or tripling scale and try a few values and see', 'start': 2225.97, 'duration': 2.79}, {'text': 'what value allows you to drive down to the learning rate fastest.', 'start': 2228.76, 'duration': 3.33}, {'text': 'Okay. Um, let me just.', 'start': 2232.09, 'duration': 2.76}, {'text': 'So I just want to visualize this in one other way,', 'start': 2234.85, 'duration': 4.17}, {'text': 'um, which is with the data.', 'start': 2239.02, 'duration': 2.235}, {'text': 'So, uh, this is this is the actual dataset.', 'start': 2241.255, 'duration': 2.67}, {'text': "Uh, they're, um, there are actually 49 points in this dataset.", 'start': 2243.925, 'duration': 3.105}, {'text': 'So m the number of training examples is 49,', 'start': 2247.03, 'duration': 3.045}, {'text': 'and so if you initialize the parameters to 0,', 'start': 2250.075, 'duration': 3.405}, {'text': 'that means, initializing your hypothesis or', 'start': 2253.48, 'duration': 3.33}, {'text': 'initializing your straight line fit to the data to be that horizontal line, right?', 'start': 2256.81, 'duration': 3.99}, {'text': 'So, if you initialize Theta 0 equals 0, Theta 1 equals 0,', 'start': 2260.8, 'duration': 4.41}, {'text': 'then your hypothesis is, you know,', 'start': 2265.21, 'duration': 2.355}, {'text': 'for any input size of house or price,', 'start': 2267.565, 'duration': 2.28}, {'text': 'the estimated price is 0, right?', 'start': 2269.845, 'duration': 2.055}, {'text': 'And so your hypothesis starts off with a horizontal line,', 'start': 2271.9, 'duration': 3.405}, {'text': 'that is whatever the input x the output y is 0.', 'start': 2275.305, 'duration': 2.97}, {'text': "And what you're doing, um,", 'start': 2278.275, 'duration': 2.55}, {'text': "as you run gradient descent is you're changing the parameters Theta, right?", 'start': 2280.825, 'duration': 5.175}, {'text': 'So the parameters went from this value to', 'start': 2286.0, 'duration': 1.8}, {'text': 'this value to this value to this value and so on.', 'start': 2287.8, 'duration': 2.73}, {'text': 'And so, the other way of visualizing gradient descent is,', 'start': 2290.53, 'duration': 3.87}, {'text': 'if gradient descent starts off with this hypothesis,', 'start': 2294.4, 'duration': 3.255}, {'text': 'with each iteration of gradient descent,', 'start': 2297.655, 'duration': 2.295}, {'text': 'you are trying to find different values of the parameters Theta,', 'start': 2299.95, 'duration': 4.61}, {'text': 'uh, that allows this straight line to fit the data better.', 'start': 2304.56, 'duration': 3.49}, {'text': 'So after one iteration of gradient descent,', 'start': 2308.05, 'duration': 2.355}, {'text': 'this is the new hypothesis,', 'start': 2310.405, 'duration': 1.485}, {'text': 'you now have different values of Theta 0 and', 'start': 2311.89, 'duration': 1.89}, {'text': 'Theta 1 that fits the data a little bit better.', 'start': 2313.78, 'duration': 2.715}, {'text': 'Um, after two iterations,', 'start': 2316.495, 'duration': 2.25}, {'text': 'you end up with that hypothesis, uh,', 'start': 2318.745, 'duration': 2.235}, {'text': "and with each iteration of gradient descent it's trying to minimize j of Theta.", 'start': 2320.98, 'duration': 4.62}, {'text': 'Is trying to minimize one half of the sum of squares errors of', 'start': 2325.6, 'duration': 2.94}, {'text': 'the hypothesis or predictions on the different examples, right?', 'start': 2328.54, 'duration': 3.63}, {'text': 'With three iterations of gradient descent, um,', 'start': 2332.17, 'duration': 2.37}, {'text': 'uh, four iterations and so on.', 'start': 2334.54, 'duration': 2.775}, {'text': 'And then and then a bunch more iterations, uh,', 'start': 2337.315, 'duration': 2.19}, {'text': 'and eventually it converges to that hypothesis,', 'start': 2339.505, 'duration': 3.495}, {'text': 'which is a pretty, pretty decent straight line fit to the data.', 'start': 2343.0, 'duration': 3.105}, {'text': 'Okay. Is there a question? Yeah, go for it.', 'start': 2346.105, 'duration': 2.295}, {'text': '[inaudible]', 'start': 2348.4, 'duration': 12.75}, {'text': 'Uh, sure.', 'start': 2361.15, 'duration': 6.9}, {'text': 'Maybe, uh, just to repeat the question.', 'start': 2368.05, 'duration': 2.1}, {'text': 'Why is the- why are you subtracting Alpha', 'start': 2370.15, 'duration': 2.7}, {'text': 'times the gradient rather than adding Alpha times the gradient?', 'start': 2372.85, 'duration': 3.24}, {'text': 'Um, let me suggest,', 'start': 2376.09, 'duration': 1.845}, {'text': 'actually let me raise the screen.', 'start': 2377.935, 'duration': 1.77}, {'text': 'Um, [NOISE] so let me suggest you work through one example.', 'start': 2379.705, 'duration': 5.325}, {'text': 'Um, uh, it turns out that if you add multiple times in a gradient,', 'start': 2385.03, 'duration': 4.17}, {'text': "you'll be going uphill rather than going downhill,", 'start': 2389.2, 'duration': 2.28}, {'text': 'and maybe one way to see that would be if, um,', 'start': 2391.48, 'duration': 3.51}, {'text': 'you know, take a quadratic function, um, excuse me.', 'start': 2394.99, 'duration': 5.1}, {'text': 'Right. If you are here,', 'start': 2400.09, 'duration': 2.115}, {'text': 'the gradient is a positive direction and you want to reduce,', 'start': 2402.205, 'duration': 3.63}, {'text': 'so this would be Theta and this will be j I guess.', 'start': 2405.835, 'duration': 2.28}, {'text': 'So you want Theta to decrease,', 'start': 2408.115, 'duration': 1.905}, {'text': 'so the gradient is positive.', 'start': 2410.02, 'duration': 1.05}, {'text': 'You wanna decrease Theta,', 'start': 2411.07, 'duration': 1.065}, {'text': 'so you want to subtract a multiple times the gradient.', 'start': 2412.135, 'duration': 2.355}, {'text': 'Um, I think maybe the best way to see that would be the work through an example yourself.', 'start': 2414.49, 'duration': 3.645}, {'text': 'Uh, uh, set j of Theta equals Theta squared and set Theta equals 1.', 'start': 2418.135, 'duration': 5.475}, {'text': 'So here at the quadratic function of the derivative is equal to 1.', 'start': 2423.61, 'duration': 2.58}, {'text': 'So you want to subtract the value from Theta rather than add.', 'start': 2426.19, 'duration': 2.805}, {'text': 'Okay? Cool. Um.', 'start': 2428.995, 'duration': 4.37}, {'text': "All right. Great. So you've now seen your first learning algorithm,", 'start': 2433.365, 'duration': 7.91}, {'text': 'um, and, you know,', 'start': 2441.275, 'duration': 2.1}, {'text': 'gradient descent and linear regression is', 'start': 2443.375, 'duration': 2.24}, {'text': 'definitely still one of the most widely used learning algorithms in the world today,', 'start': 2445.615, 'duration': 3.355}, {'text': 'and if you implement this- if you,', 'start': 2448.97, 'duration': 1.395}, {'text': 'if you, if you implement this today,', 'start': 2450.365, 'duration': 1.98}, {'text': 'right, you could use this for,', 'start': 2452.345, 'duration': 1.77}, {'text': 'for some actually pretty, pretty decent purposes.', 'start': 2454.115, 'duration': 2.505}, {'text': 'Um, now, I wanna I give this algorithm one other name.', 'start': 2456.62, 'duration': 8.745}, {'text': 'Uh, so our gradient descent algorithm here, um,', 'start': 2465.365, 'duration': 4.755}, {'text': 'calculates this derivative by summing over', 'start': 2470.12, 'duration': 3.99}, {'text': 'your entire training set m. And so sometimes this version of gradient descent,', 'start': 2474.11, 'duration': 4.98}, {'text': 'has another name, which is batch gradient descent.', 'start': 2479.09, 'duration': 3.885}, {'text': 'Oops. All right', 'start': 2482.975, 'duration': 10.47}, {'text': 'and the term batch,', 'start': 2493.445, 'duration': 1.77}, {'text': 'um, you know- and again- I think in machine learning, uh,', 'start': 2495.215, 'duration': 3.15}, {'text': "our whole committee, we just make up names and stuff and sometimes the names aren't great.", 'start': 2498.365, 'duration': 3.405}, {'text': 'But the- the term batch gradient descent refers to that,', 'start': 2501.77, 'duration': 3.585}, {'text': 'you look at the entire training set,', 'start': 2505.355, 'duration': 1.89}, {'text': 'all 49 examples in the example I just had on, uh, PowerPoint.', 'start': 2507.245, 'duration': 3.585}, {'text': 'You know, you- you think of all for 49 examples as one batch of data,', 'start': 2510.83, 'duration': 3.93}, {'text': "I'm gonna process all the data as a batch,", 'start': 2514.76, 'duration': 2.475}, {'text': 'so hence the name batch gradient descent.', 'start': 2517.235, 'duration': 2.91}, {'text': 'The disadvantage of batch gradient descent is that if you have a giant dataset,', 'start': 2520.145, 'duration': 5.04}, {'text': 'if you have, um,', 'start': 2525.185, 'duration': 1.425}, {'text': "and- and in the era of big data we're really,", 'start': 2526.61, 'duration': 2.34}, {'text': 'moving to larger and larger datasets,', 'start': 2528.95, 'duration': 2.115}, {'text': "so I've used, you know,", 'start': 2531.065, 'duration': 1.305}, {'text': 'we train machine learning models of like hundreds of millions of examples.', 'start': 2532.37, 'duration': 3.81}, {'text': 'And- and if you are trying to- if you have, uh,', 'start': 2536.18, 'duration': 3.0}, {'text': 'if you download the US census database,', 'start': 2539.18, 'duration': 2.07}, {'text': 'if your data, the United States census,', 'start': 2541.25, 'duration': 2.28}, {'text': "that's a very large dataset.", 'start': 2543.53, 'duration': 1.695}, {'text': 'And you wanna predict housing prices,', 'start': 2545.225, 'duration': 1.56}, {'text': 'from all across the United States,', 'start': 2546.785, 'duration': 1.635}, {'text': 'um, that- that- that may have a dataset with many- many millions of examples.', 'start': 2548.42, 'duration': 4.92}, {'text': 'And the disadvantage of batch gradient descent is that,', 'start': 2553.34, 'duration': 3.48}, {'text': 'in order to make one update,', 'start': 2556.82, 'duration': 2.7}, {'text': 'to your parameters, in order to even take a single step of gradient descent,', 'start': 2559.52, 'duration': 4.02}, {'text': 'you need to calculate, this sum.', 'start': 2563.54, 'duration': 3.165}, {'text': 'And if m is say a million or 10 million or 100 million,', 'start': 2566.705, 'duration': 4.355}, {'text': 'you need to scan through your entire database,', 'start': 2571.06, 'duration': 3.045}, {'text': 'scan through your entire dataset and calculate this for,', 'start': 2574.105, 'duration': 4.155}, {'text': 'you know, 100 million examples and sum it up.', 'start': 2578.26, 'duration': 2.965}, {'text': 'And so every single step of gradient descent', 'start': 2581.225, 'duration': 2.145}, {'text': "becomes very slow because you're scanning over,", 'start': 2583.37, 'duration': 2.835}, {'text': "you're reading over, right,", 'start': 2586.205, 'duration': 1.665}, {'text': 'like 100 million training examples, uh, uh,', 'start': 2587.87, 'duration': 3.03}, {'text': 'and uh, uh, before you can even, you know,', 'start': 2590.9, 'duration': 2.145}, {'text': 'make one tiny little step of gradient descent.', 'start': 2593.045, 'duration': 2.61}, {'text': 'Okay, um, yeah, and by the way,', 'start': 2595.655, 'duration': 2.22}, {'text': "I think- I feel like in today's era of", 'start': 2597.875, 'duration': 2.385}, {'text': "big data people start to lose intuitions about what's a big data-set.", 'start': 2600.26, 'duration': 2.94}, {'text': "I think even by today's standards,", 'start': 2603.2, 'duration': 1.575}, {'text': 'like a hundred million examples is still very big, right,', 'start': 2604.775, 'duration': 2.505}, {'text': 'I- I rarely- only rarely use a hundred million examples.', 'start': 2607.28, 'duration': 3.375}, {'text': "Um, I don't know,", 'start': 2610.655, 'duration': 1.755}, {'text': "maybe in a few years we'll look back on", 'start': 2612.41, 'duration': 1.83}, {'text': 'a hundred million examples and say that was really small,', 'start': 2614.24, 'duration': 1.86}, {'text': 'but at least today. Uh, yeah.', 'start': 2616.1, 'duration': 3.345}, {'text': 'So the main disadvantage of batch gradient descent is,', 'start': 2619.445, 'duration': 4.29}, {'text': 'every single step of gradient descent requires that you read through, you know,', 'start': 2623.735, 'duration': 3.825}, {'text': 'your entire data-set, maybe terabytes of data-sets maybe- maybe- maybe, uh,', 'start': 2627.56, 'duration': 4.155}, {'text': 'tens or hundreds of terabytes of data,', 'start': 2631.715, 'duration': 1.935}, {'text': 'uh, before you can even update the parameters just once.', 'start': 2633.65, 'duration': 3.18}, {'text': 'And if gradient descent needs, you know,', 'start': 2636.83, 'duration': 2.445}, {'text': 'hundreds of iterations to converge,', 'start': 2639.275, 'duration': 2.415}, {'text': "then you'll be scanning through your entire data-set hundreds of times.", 'start': 2641.69, 'duration': 3.6}, {'text': 'Right, or-or and then sometimes we train,', 'start': 2645.29, 'duration': 2.34}, {'text': 'our algorithms with thousands or tens of thousands of iterations.', 'start': 2647.63, 'duration': 2.73}, {'text': 'And so- so this- this gets expensive.', 'start': 2650.36, 'duration': 3.39}, {'text': "So there's an alternative to batch gradient descent. Um, and", 'start': 2653.75, 'duration': 5.28}, {'text': 'let me just write out the algorithm here that we can talk about it,', 'start': 2659.03, 'duration': 2.775}, {'text': 'which is going to repeatedly do this.', 'start': 2661.805, 'duration': 4.455}, {'text': '[NOISE] Oops, okay. Um, so this algorithm,', 'start': 2666.26, 'duration': 29.365}, {'text': 'which is called stochastic gradient descent.', 'start': 2695.625, 'duration': 2.845}, {'text': '[NOISE] Um, instead of scanning through', 'start': 2698.47, 'duration': 10.845}, {'text': 'all million examples before you update the parameters theta even a little bit,', 'start': 2709.315, 'duration': 5.07}, {'text': 'in stochastic gradient descent,', 'start': 2714.385, 'duration': 1.8}, {'text': 'instead, in the inner loop of the algorithm,', 'start': 2716.185, 'duration': 2.19}, {'text': 'you loop through j equals 1 through m of taking a gradient descent step', 'start': 2718.375, 'duration': 5.185}, {'text': 'using, the derivative of just one single example of just that,', 'start': 2723.56, 'duration': 4.605}, {'text': 'uh, one example, ah,', 'start': 2728.165, 'duration': 1.635}, {'text': "oh, excuse me it's through i, right.", 'start': 2729.8, 'duration': 2.505}, {'text': 'Yeah, so let i go from 1 to m,', 'start': 2732.305, 'duration': 2.685}, {'text': 'and update theta j for every j.', 'start': 2734.99, 'duration': 2.835}, {'text': 'So you update this for j equals 1 through n, update theta j,', 'start': 2737.825, 'duration': 4.95}, {'text': 'using this derivative that when now this derivative', 'start': 2742.775, 'duration': 4.035}, {'text': 'is taken just with respect to one training example- example I.', 'start': 2746.81, 'duration': 4.35}, {'text': "Okay, um, I'll- I'll just- alright and I guess you update this for every j.", 'start': 2751.16, 'duration': 8.35}, {'text': 'Okay, and so, let me just draw a picture of what this algorithm is doing.', 'start': 2762.58, 'duration': 7.36}, {'text': 'If um, this is the contour,', 'start': 2769.94, 'duration': 3.825}, {'text': 'like the one you saw just now.', 'start': 2773.765, 'duration': 2.37}, {'text': 'So the axes are, uh,', 'start': 2776.135, 'duration': 2.25}, {'text': 'theta 0 and theta 1,', 'start': 2778.385, 'duration': 1.755}, {'text': 'and the height of the surface right, denote the contours as j of theta.', 'start': 2780.14, 'duration': 3.6}, {'text': 'With stochastic gradient descent,', 'start': 2783.74, 'duration': 2.1}, {'text': 'what you do is you initialize the parameters somewhere.', 'start': 2785.84, 'duration': 2.775}, {'text': 'And then you will look at your first training example.', 'start': 2788.615, 'duration': 3.18}, {'text': 'Hey, lets just look at one house,', 'start': 2791.795, 'duration': 1.485}, {'text': 'and see if you can predict that house as better,', 'start': 2793.28, 'duration': 2.19}, {'text': 'and you modify the parameters to', 'start': 2795.47, 'duration': 2.25}, {'text': 'increase the accuracy where you predict the price of that one house.', 'start': 2797.72, 'duration': 3.12}, {'text': "And because you're fitting the data just for one house, um, you know,", 'start': 2800.84, 'duration': 3.975}, {'text': 'maybe you end up improving the parameters a little bit,', 'start': 2804.815, 'duration': 3.975}, {'text': 'but not quite going in the most direct direction downhill.', 'start': 2808.79, 'duration': 4.14}, {'text': 'And you go look at the second house and say,', 'start': 2812.93, 'duration': 1.5}, {'text': "hey, let's try to fit that house better.", 'start': 2814.43, 'duration': 1.755}, {'text': 'And then you update the parameters.', 'start': 2816.185, 'duration': 1.425}, {'text': 'And you look at third house, fourth house.', 'start': 2817.61, 'duration': 2.325}, {'text': 'Right, and so as you run stochastic gradient descent,', 'start': 2819.935, 'duration': 4.32}, {'text': 'it takes a slightly noisy, slightly random path.', 'start': 2824.255, 'duration': 3.345}, {'text': 'Uh, but on average,', 'start': 2827.6, 'duration': 1.905}, {'text': "it's headed toward the global minimum, okay.", 'start': 2829.505, 'duration': 4.485}, {'text': 'So as you run stochastic gradient descent-', 'start': 2833.99, 'duration': 3.21}, {'text': 'stochastic gradient descent will actually, never quite converge.', 'start': 2837.2, 'duration': 3.72}, {'text': 'In- with- with batch gradient descent,', 'start': 2840.92, 'duration': 1.68}, {'text': 'it kind of went to the global minimum and stopped right,', 'start': 2842.6, 'duration': 4.185}, {'text': "uh, with stochastic gradient descent even as you won't run it,", 'start': 2846.785, 'duration': 2.385}, {'text': "the parameters will oscillate and won't ever quite converge because you're always", 'start': 2849.17, 'duration': 4.2}, {'text': 'running around looking at different houses and trying to do better than', 'start': 2853.37, 'duration': 2.37}, {'text': 'just that one house- and that one house- and that one house.', 'start': 2855.74, 'duration': 2.61}, {'text': 'Uh, but when you have a very large data-set,', 'start': 2858.35, 'duration': 3.72}, {'text': 'stochastic gradient descent, allows', 'start': 2862.07, 'duration': 2.58}, {'text': 'your implementation- allows you algorithm to make much faster progress.', 'start': 2864.65, 'duration': 3.75}, {'text': 'Uh, and so, um, uh,', 'start': 2868.4, 'duration': 3.75}, {'text': 'uh- and so when you have very large data-sets,', 'start': 2872.15, 'duration': 2.1}, {'text': 'stochastic gradient descent is used much more in practice than batch gradient descent.', 'start': 2874.25, 'duration': 4.62}, {'text': '[BACKGROUND]', 'start': 2878.87, 'duration': 13.5}, {'text': 'Uh, yeah, is it possible to start with stochastic gradient descent and then switch over to batch gradient descent?', 'start': 2892.37, 'duration': 3.39}, {'text': 'Yes, it is.', 'start': 2895.76, 'duration': 2.16}, {'text': "So, uh, boy, something that wasn't talked about in this class,", 'start': 2897.92, 'duration': 4.32}, {'text': "it's talked about in CS230 is Mini-batch gradient descent, where, um,", 'start': 2902.24, 'duration': 3.69}, {'text': "you don't- where you use say", 'start': 2905.93, 'duration': 1.785}, {'text': 'a hundred examples at a time rather than one example at a time.', 'start': 2907.715, 'duration': 2.91}, {'text': "And so- uh, so that's another algorithm that I should use more often in practice.", 'start': 2910.625, 'duration': 4.56}, {'text': 'I think people rarely- actually,', 'start': 2915.185, 'duration': 2.04}, {'text': 'so- so in practice, you know,', 'start': 2917.225, 'duration': 2.685}, {'text': 'when your dataset is large, we rarely,', 'start': 2919.91, 'duration': 3.855}, {'text': 'ever switch to batch gradient descent,', 'start': 2923.765, 'duration': 2.91}, {'text': 'because batch gradient descent is just so slow, right.', 'start': 2926.675, 'duration': 2.505}, {'text': "So, I-I know I'm thinking through concrete examples of problems I've worked on.", 'start': 2929.18, 'duration': 5.355}, {'text': 'And I think that what- maybe actually maybe- I think that uh,', 'start': 2934.535, 'duration': 4.825}, {'text': 'for a lot of- for- for modern machine learning,', 'start': 2939.58, 'duration': 3.28}, {'text': 'where you have- if you have very- very large data sets, right so you know,', 'start': 2942.86, 'duration': 2.97}, {'text': "whether- if you're building a speech recognition system,", 'start': 2945.83, 'duration': 2.34}, {'text': 'you might have like a terabyte of data,', 'start': 2948.17, 'duration': 2.175}, {'text': 'right, and so, um,', 'start': 2950.345, 'duration': 1.995}, {'text': "it's so expensive to scan through a terabyte of data just reading it from disk,", 'start': 2952.34, 'duration': 4.905}, {'text': "right it's so expensive that you would", 'start': 2957.245, 'duration': 2.595}, {'text': 'probably never even run one iteration of batch gradient descent.', 'start': 2959.84, 'duration': 3.42}, {'text': 'Uh, and it turns out the- the-', 'start': 2963.26, 'duration': 2.6}, {'text': "there's one- one huge saving grace of stochastic gradient descent is,", 'start': 2965.86, 'duration': 3.31}, {'text': "um, let's say you run stochastic gradient descent, right,", 'start': 2969.17, 'duration': 3.6}, {'text': "and, you know, you end up with this parameter and that's the parameter you use,", 'start': 2972.77, 'duration': 6.0}, {'text': 'for your machine learning system,', 'start': 2978.77, 'duration': 1.755}, {'text': 'rather than the global optimum.', 'start': 2980.525, 'duration': 2.01}, {'text': 'It turns out that parameter is actually not that bad, right,', 'start': 2982.535, 'duration': 3.045}, {'text': "you- you probably make perfectly fine predictions even if you don't", 'start': 2985.58, 'duration': 2.88}, {'text': 'quite get to the like the global- global minimum.', 'start': 2988.46, 'duration': 3.33}, {'text': "So, uh, what you said I think it's a fine thing to do, no harm trying it.", 'start': 2991.79, 'duration': 5.085}, {'text': 'Although in practice uh,', 'start': 2996.875, 'duration': 2.015}, {'text': 'uh, in practice we', 'start': 2998.89, 'duration': 2.16}, {'text': "don't bother, I think in practice we usually use stochastic gradient descent.", 'start': 3001.05, 'duration': 2.82}, {'text': 'The thing that actually is more common,', 'start': 3003.87, 'duration': 1.86}, {'text': 'is to slowly decrease the learning rate.', 'start': 3005.73, 'duration': 1.77}, {'text': 'So just keep using stochastic gradient descent,', 'start': 3007.5, 'duration': 1.92}, {'text': 'but reduce the learning rate over time.', 'start': 3009.42, 'duration': 1.86}, {'text': 'So it takes smaller and smaller steps.', 'start': 3011.28, 'duration': 1.58}, {'text': 'So if you do that, then what happens is the size of the oscillations would decrease.', 'start': 3012.86, 'duration': 4.97}, {'text': 'Uh, and so you end up oscillating or bouncing around the smaller region.', 'start': 3017.83, 'duration': 3.93}, {'text': 'So wherever you end up,', 'start': 3021.76, 'duration': 1.29}, {'text': 'may not be the global- global minimum,', 'start': 3023.05, 'duration': 2.22}, {'text': "but at least it'll- be it'll be closer to the global minimum.", 'start': 3025.27, 'duration': 2.76}, {'text': 'Yeah, so decreasing the learning rate is used much more often.', 'start': 3028.03, 'duration': 3.375}, {'text': 'Cool. Question. Yeah. [BACKGROUND]', 'start': 3031.405, 'duration': 8.305}, {'text': 'Oh sure, when do you stop with stochastic gradient descent?', 'start': 3039.71, 'duration': 2.64}, {'text': 'Uh, uh, plot to j of theta, uh, over time.', 'start': 3042.35, 'duration': 4.485}, {'text': "So j of theta is a cost function that you're trying to drag down.", 'start': 3046.835, 'duration': 3.015}, {'text': 'So monitor j of theta as,', 'start': 3049.85, 'duration': 2.535}, {'text': 'you know, is going down over time,', 'start': 3052.385, 'duration': 1.62}, {'text': 'and then if it looks like it stopped going down,', 'start': 3054.005, 'duration': 2.25}, {'text': 'then you can say, "Oh,', 'start': 3056.255, 'duration': 1.065}, {'text': 'it looks like it looks like it stopped going down," then you stop training.', 'start': 3057.32, 'duration': 2.415}, {'text': 'Although- and then- ah, uh, you know,', 'start': 3059.735, 'duration': 2.34}, {'text': 'one nice thing about linear regression is that it has no local optimum and so,', 'start': 3062.075, 'duration': 4.665}, {'text': 'um, uh, it- you run into these convergence debugging types of issues less often.', 'start': 3066.74, 'duration': 6.855}, {'text': "Where you're training highly non-linear things like neural networks,", 'start': 3073.595, 'duration': 2.955}, {'text': 'we should talk about later in CS229 as well.', 'start': 3076.55, 'duration': 2.205}, {'text': 'Uh, these issues become more acute.', 'start': 3078.755, 'duration': 3.385}, {'text': 'Cool. Okay, great.', 'start': 3082.45, 'duration': 5.095}, {'text': 'So, um, uh, yeah.', 'start': 3087.545, 'duration': 2.775}, {'text': '[BACKGROUND].', 'start': 3090.32, 'duration': 5.34}, {'text': 'Oh, would your learning rate be 1 over n times linear regressions then? Not really,', 'start': 3095.66, 'duration': 3.06}, {'text': "it's usually much bigger than that.", 'start': 3098.72, 'duration': 2.13}, {'text': 'Uh, uh, yeah, because if your learning rate was 1 over n times', 'start': 3100.85, 'duration': 4.23}, {'text': "that of what you'd use with batch gradient descent", 'start': 3105.08, 'duration': 1.77}, {'text': 'then it would end up being as slow as batch gradient descent,', 'start': 3106.85, 'duration': 2.04}, {'text': "so it's usually much bigger.", 'start': 3108.89, 'duration': 2.08}, {'text': "Okay. So, um, so that's stochastic gradient descent and- and- so I'll tell you what I do.", 'start': 3111.01, 'duration': 8.185}, {'text': 'If- if you have a relatively small dataset,', 'start': 3119.195, 'duration': 2.22}, {'text': "you know, if you have- if you have, I don't know,", 'start': 3121.415, 'duration': 1.725}, {'text': 'like hundreds of examples maybe thousands of examples where,', 'start': 3123.14, 'duration': 3.39}, {'text': "uh, it's computationally efficient to do batch gradient descent.", 'start': 3126.53, 'duration': 3.495}, {'text': "If batch gradient descent doesn't cost too much,", 'start': 3130.025, 'duration': 2.325}, {'text': 'I would almost always just use', 'start': 3132.35, 'duration': 1.68}, {'text': "batch gradient descent because it's one less thing to fiddle with, right?", 'start': 3134.03, 'duration': 2.655}, {'text': "It's just one less thing to have to worry about,", 'start': 3136.685, 'duration': 1.995}, {'text': 'uh, the parameters oscillating,', 'start': 3138.68, 'duration': 1.98}, {'text': 'but your dataset is too large that', 'start': 3140.66, 'duration': 2.88}, {'text': 'batch gradient descent becomes prohibit- prohibitively slow,', 'start': 3143.54, 'duration': 3.33}, {'text': 'then almost everyone will use, you know,', 'start': 3146.87, 'duration': 2.49}, {'text': 'stochastic gradient descent or whatever more like stochastic gradient descent, okay?', 'start': 3149.36, 'duration': 5.02}, {'text': 'All right, so, um, gradient descent,', 'start': 3166.99, 'duration': 6.595}, {'text': 'both batch gradient descent and stochastic gradient descent is', 'start': 3173.585, 'duration': 3.54}, {'text': 'an iterative algorithm meaning that you have to take multiple steps to get to,', 'start': 3177.125, 'duration': 4.515}, {'text': 'you know, get near hopefully the global optimum.', 'start': 3181.64, 'duration': 3.12}, {'text': 'It turns out there is another algorithm,', 'start': 3184.76, 'duration': 2.61}, {'text': 'uh, and- and, um,', 'start': 3187.37, 'duration': 2.1}, {'text': "for many other algorithms we'll talk about in this course", 'start': 3189.47, 'duration': 2.805}, {'text': 'including generalized linear models and neural networks and a few other algorithms, uh,', 'start': 3192.275, 'duration': 4.275}, {'text': "you will have to use gradient descent and so- and so we'll see gradient descent,", 'start': 3196.55, 'duration': 4.185}, {'text': 'you know, as we develop multiple different algorithms later this quarter.', 'start': 3200.735, 'duration': 4.485}, {'text': 'It turns out that for the special case of linear regression, uh, uh,', 'start': 3205.22, 'duration': 4.74}, {'text': "and I mean linear regression but not the algorithm we'll talk about next Monday,", 'start': 3209.96, 'duration': 3.315}, {'text': "not the algorithm we'll talk about next Wednesday,", 'start': 3213.275, 'duration': 1.395}, {'text': "but if the algorithm you're using is linear regression and exactly linear regression.", 'start': 3214.67, 'duration': 4.5}, {'text': "It turns out that there's a way to, uh,", 'start': 3219.17, 'duration': 2.73}, {'text': 'solve for the optimal value of the parameters theta to just jump in', 'start': 3221.9, 'duration': 4.29}, {'text': 'one step to the global optimum without needing to use an iterative algorithm,', 'start': 3226.19, 'duration': 5.1}, {'text': "right, and this- this one I'm gonna present next is called the normal equation.", 'start': 3231.29, 'duration': 4.29}, {'text': 'It works only for linear regression,', 'start': 3235.58, 'duration': 1.8}, {'text': "doesn't work for any of the other algorithms I talk about later this quarter.", 'start': 3237.38, 'duration': 2.895}, {'text': 'But [NOISE] um, uh,', 'start': 3240.275, 'duration': 7.695}, {'text': 'let me quickly show you the derivation of that.', 'start': 3247.97, 'duration': 4.305}, {'text': 'And, um, what I want to do is, uh,', 'start': 3252.275, 'duration': 3.96}, {'text': 'give you a flavor of how to derive', 'start': 3256.235, 'duration': 3.255}, {'text': 'the normal equation and where you end up with is you know,', 'start': 3259.49, 'duration': 4.35}, {'text': 'wha- what- what I hope to do is end up with a formula that lets you', 'start': 3263.84, 'duration': 3.27}, {'text': 'say theta equals some stuff where you just', 'start': 3267.11, 'duration': 4.17}, {'text': 'set theta equals to that and in one step with a few matrix multiplications you', 'start': 3271.28, 'duration': 4.35}, {'text': 'end up with the optimal value of theta that lands you right at the global optimum,', 'start': 3275.63, 'duration': 4.68}, {'text': 'right, just like that, just in one step.', 'start': 3280.31, 'duration': 2.175}, {'text': "Okay. Um, and if you've taken, you know,", 'start': 3282.485, 'duration': 3.675}, {'text': 'advanced linear algebra courses before or something,', 'start': 3286.16, 'duration': 2.355}, {'text': 'you may have seen, um,', 'start': 3288.515, 'duration': 2.04}, {'text': 'this formula for linear regression.', 'start': 3290.555, 'duration': 2.82}, {'text': 'Wha- what a lot of linear algebra classes do', 'start': 3293.375, 'duration': 2.805}, {'text': 'is, what some linear algebra classes do is cover the board with,', 'start': 3296.18, 'duration': 4.305}, {'text': 'you know, pages and pages of matrix derivatives.', 'start': 3300.485, 'duration': 2.85}, {'text': 'Um, what I wanna do is describe to you', 'start': 3303.335, 'duration': 2.985}, {'text': 'a matrix derivative notation that allows you to derive', 'start': 3306.32, 'duration': 3.81}, {'text': 'the normal equation in roughly four lines of linear algebra,', 'start': 3310.13, 'duration': 3.99}, {'text': 'uh, rather than some pages and pages of', 'start': 3314.12, 'duration': 1.95}, {'text': "linear algebra and in the work I've done in machine learning you know,", 'start': 3316.07, 'duration': 3.975}, {'text': 'sometimes notation really matters, right.', 'start': 3320.045, 'duration': 1.875}, {'text': 'If you have the right notation you can solve some problems', 'start': 3321.92, 'duration': 2.55}, {'text': 'much more easily and what I wanna do is,', 'start': 3324.47, 'duration': 2.76}, {'text': 'um, uh, define this uh,', 'start': 3327.23, 'duration': 2.1}, {'text': "matrix linear algebra notation and then I don't wanna do all the steps of the derivation,", 'start': 3329.33, 'duration': 6.3}, {'text': 'I wanna give you- give you a sense of the flavor of what it looks like and then,', 'start': 3335.63, 'duration': 3.81}, {'text': "um, I'll ask you to,", 'start': 3339.44, 'duration': 1.905}, {'text': 'uh, uh, get a lot of details yourself, um,', 'start': 3341.345, 'duration': 2.685}, {'text': 'in the- in the lecture notes where we', 'start': 3344.03, 'duration': 2.79}, {'text': 'work out everything in more detail than I want to do algebra in class.', 'start': 3346.82, 'duration': 3.33}, {'text': "And, um, in problem set one you'll get to practice using this yourself to- to- to-,", 'start': 3350.15, 'duration': 5.28}, {'text': 'you know, derive some additional things.', 'start': 3355.43, 'duration': 1.65}, {'text': "I've- I've found this notation really convenient,", 'start': 3357.08, 'duration': 2.804}, {'text': 'right, for deriving learning algorithms.', 'start': 3359.884, 'duration': 2.626}, {'text': "Okay. So, um, I'm going to use the following notation.", 'start': 3362.51, 'duration': 5.925}, {'text': 'Um, so J, right.', 'start': 3368.435, 'duration': 5.175}, {'text': "There's a function mapping from parameters to the real numbers.", 'start': 3373.61, 'duration': 3.375}, {'text': "So I'm going to define this- this is the derivative of J of theta with respect to theta,", 'start': 3376.985, 'duration': 10.095}, {'text': 'where- remember theta is a three-dimensional vector says R3,', 'start': 3387.08, 'duration': 5.61}, {'text': "or actually it's R n+1, right.", 'start': 3392.69, 'duration': 2.295}, {'text': 'If you have, uh, two features to the house if n=2,', 'start': 3394.985, 'duration': 3.165}, {'text': 'then theta was 3 dimensional,', 'start': 3398.15, 'duration': 1.769}, {'text': "it's n+1 dimensional so it's a vector.", 'start': 3399.919, 'duration': 2.101}, {'text': "And so I'm gonna define the derivative with respect to theta of J of theta as follows.", 'start': 3402.02, 'duration': 6.33}, {'text': 'Um, this is going to be itself a', 'start': 3408.35, 'duration': 2.265}, {'text': '3 by 1 vector', 'start': 3410.615, 'duration': 2.475}, {'text': '[NOISE].', 'start': 3413.09, 'duration': 8.19}, {'text': 'Okay, so I hope this notation is clear.', 'start': 3421.28, 'duration': 2.685}, {'text': 'So this is a three-dimensional vector with, uh, three components.', 'start': 3423.965, 'duration': 5.875}, {'text': "Alright so that's what I guess I'm.", 'start': 3432.37, 'duration': 5.3}, {'text': "So that's the first component is a vector,", 'start': 3440.5, 'duration': 2.83}, {'text': "there's a second and there's a third.", 'start': 3443.33, 'duration': 1.755}, {'text': "It's the partial derivative of J with respect to each of the three elements.", 'start': 3445.085, 'duration': 5.755}, {'text': 'Um, and more generally,', 'start': 3451.03, 'duration': 4.73}, {'text': "in the notation we'll use,", 'start': 3466.69, 'duration': 2.515}, {'text': 'um, let me give you an example.', 'start': 3469.205, 'duration': 3.345}, {'text': "Um, uh, let's say that a is a matrix.", 'start': 3472.55, 'duration': 4.755}, {'text': "So let's say that a is a two-by-two matrix.", 'start': 3477.305, 'duration': 4.935}, {'text': 'Then, um, you can have a function,', 'start': 3482.24, 'duration': 3.57}, {'text': "right, so let's say a is, you know,", 'start': 3485.81, 'duration': 2.655}, {'text': 'A1-1, A1-2, A2-1 and A2-2, right.', 'start': 3488.465, 'duration': 6.18}, {'text': 'So A is a two-by-two matrix.', 'start': 3494.645, 'duration': 1.92}, {'text': 'Then you might have some function um,', 'start': 3496.565, 'duration': 3.81}, {'text': 'of a matrix A right,', 'start': 3500.375, 'duration': 2.295}, {'text': "then that's a real number.", 'start': 3502.67, 'duration': 1.05}, {'text': 'So maybe f maps from A 2-by-2 to,', 'start': 3503.72, 'duration': 5.28}, {'text': "uh, excuse me, R 2-by-2, it's a real number.", 'start': 3509.0, 'duration': 3.9}, {'text': 'So, um, uh, and so for example,', 'start': 3512.9, 'duration': 4.2}, {'text': 'if f of A equals A11 plus A12 squared,', 'start': 3517.1, 'duration': 4.485}, {'text': 'then f of, you know, 5,', 'start': 3521.585, 'duration': 3.525}, {'text': '6, 7, 8 would be equal to I guess 5 plus 6 squared, right.', 'start': 3525.11, 'duration': 7.395}, {'text': 'So as we derive this,', 'start': 3532.505, 'duration': 1.905}, {'text': "we'll be working a little bit with functions that map from", 'start': 3534.41, 'duration': 2.01}, {'text': 'matrices to real numbers and this is just one made up', 'start': 3536.42, 'duration': 3.09}, {'text': 'example of a function that', 'start': 3539.51, 'duration': 1.62}, {'text': 'inputs a matrix and maps the matrix, maps the values of matrix to a real number.', 'start': 3541.13, 'duration': 4.335}, {'text': 'And when you have a matrix function like this,', 'start': 3545.465, 'duration': 4.29}, {'text': "I'm going to define the derivative with respect to A of f of A to be", 'start': 3549.755, 'duration': 7.395}, {'text': 'equal to itself a matrix where the derivative of f of A with respect to the matrix A.', 'start': 3557.15, 'duration': 7.73}, {'text': 'Uh, this itself will be a matrix with the same dimension of a and the elements of', 'start': 3564.88, 'duration': 8.23}, {'text': 'this are the derivative with respect to the individual elements.', 'start': 3573.11, 'duration': 10.06}, {'text': 'Actually, let me just write it like this.', 'start': 3584.83, 'duration': 2.47}, {'text': '[NOISE]', 'start': 3587.3, 'duration': 14.58}, {'text': 'Okay. So if A was a 2-by-2 matrix', 'start': 3601.88, 'duration': 3.0}, {'text': 'then the derivative of F of A with respect to A is itself', 'start': 3604.88, 'duration': 3.12}, {'text': 'a 2-by-2 matrix and you compute this 2-by-2 matrix just by looking at F and taking,', 'start': 3608.0, 'duration': 6.405}, {'text': 'uh, derivatives with respect to', 'start': 3614.405, 'duration': 2.355}, {'text': 'the different elements and plugging them into the different,', 'start': 3616.76, 'duration': 2.445}, {'text': 'the different elements of this matrix.', 'start': 3619.205, 'duration': 1.995}, {'text': 'Okay. Um, and so in this particular example,', 'start': 3621.2, 'duration': 3.63}, {'text': 'I guess the derivative respect to A of F of A.', 'start': 3624.83, 'duration': 3.495}, {'text': 'This would be, um, [NOISE] right,', 'start': 3628.325, 'duration': 9.315}, {'text': 'it would be- it would be that.', 'start': 3637.64, 'duration': 1.2}, {'text': 'Ah and I got these four numbers by taking, um,', 'start': 3638.84, 'duration': 6.045}, {'text': 'the definition of F and taking', 'start': 3644.885, 'duration': 3.045}, {'text': 'the derivative with respect to A_1, 1 and plugging that here.', 'start': 3647.93, 'duration': 3.27}, {'text': 'Ah, taking the derivative with respect to A_1,2 and', 'start': 3651.2, 'duration': 5.73}, {'text': 'plugging that here and taking the derivative with respect to', 'start': 3656.93, 'duration': 2.76}, {'text': 'the remaining elements and plugging them here which- which was 0.', 'start': 3659.69, 'duration': 3.435}, {'text': "Okay? So that's the definition of a matrix derivative. Yeah?", 'start': 3663.125, 'duration': 4.515}, {'text': '[inaudible].', 'start': 3667.64, 'duration': 5.07}, {'text': "Oh, yes. We're just using the definition for a vector.", 'start': 3672.71, 'duration': 2.1}, {'text': 'Ah, N by 1 or N by 1 matrix.', 'start': 3674.81, 'duration': 1.89}, {'text': 'Yes. And in fact that definition and', 'start': 3676.7, 'duration': 2.88}, {'text': 'this definition for the derivative of J with respect to Theta these are consistent.', 'start': 3679.58, 'duration': 4.035}, {'text': 'So if you apply that definition to a column vector,', 'start': 3683.615, 'duration': 3.015}, {'text': 'treating a column vector as an N by 1 matrix or N,', 'start': 3686.63, 'duration': 3.105}, {'text': 'I guess it would be N plus 1 by 1 matrix', 'start': 3689.735, 'duration': 2.115}, {'text': 'then that- that specializes to what we described here.', 'start': 3691.85, 'duration': 3.36}, {'text': '[NOISE]', 'start': 3695.21, 'duration': 14.58}, {'text': "All right. So, um, let's see.", 'start': 3709.79, 'duration': 7.755}, {'text': "Okay. So, um, I want to leave the details of the lecture notes because there's", 'start': 3717.545, 'duration': 6.795}, {'text': "more lines of algebra which I won't do but it'll give you an overview", 'start': 3724.34, 'duration': 3.21}, {'text': '[NOISE] of what the derivation of the normal equation looks like.', 'start': 3727.55, 'duration': 3.705}, {'text': 'Um, so onto this definition of a derivative of a- of a matrix,', 'start': 3731.255, 'duration': 5.535}, {'text': "um, the broad outline of what we're going to do is we're going to take J of Theta.", 'start': 3736.79, 'duration': 6.285}, {'text': "Right. That's the cost function.", 'start': 3743.075, 'duration': 2.22}, {'text': 'Um, take the derivative with respect to Theta.', 'start': 3745.295, 'duration': 5.03}, {'text': 'Right. Ah, since Theta is a vector so you want to take', 'start': 3750.325, 'duration': 3.645}, {'text': 'the derivative with respect to Theta and you know well,', 'start': 3753.97, 'duration': 3.06}, {'text': 'how do you minimize a function?', 'start': 3757.03, 'duration': 1.23}, {'text': 'You take derivatives with [NOISE] respect to Theta and set it equal to 0.', 'start': 3758.26, 'duration': 4.96}, {'text': 'And then you solve for the value of Theta so that the derivative is 0.', 'start': 3763.22, 'duration': 3.495}, {'text': 'Right. The- the minimum, you know,', 'start': 3766.715, 'duration': 1.395}, {'text': 'the maximum and minimum of a function is where the derivative is equal to 0.', 'start': 3768.11, 'duration': 2.76}, {'text': 'So- so how you derive the normal equation is take this vector.', 'start': 3770.87, 'duration': 3.855}, {'text': 'Ah, so J of Theta maps from a vector to a real number.', 'start': 3774.725, 'duration': 3.615}, {'text': "So we'll, take the derivatives respect to Theta set it to 0,0 and solve", 'start': 3778.34, 'duration': 3.9}, {'text': 'for Theta and then we end up with a formula for Theta that lets you just,', 'start': 3782.24, 'duration': 3.99}, {'text': 'um, ah, you know,', 'start': 3786.23, 'duration': 2.28}, {'text': 'immediately go to the global minimum of the- of the cost function J of Theta.', 'start': 3788.51, 'duration': 4.845}, {'text': 'And, and a lot of the build up,', 'start': 3793.355, 'duration': 2.025}, {'text': 'a lot of this notation is, you know,', 'start': 3795.38, 'duration': 2.025}, {'text': 'is there- what does this mean and is there', 'start': 3797.405, 'duration': 1.815}, {'text': 'an easy way to compute the derivative of J of Theta?', 'start': 3799.22, 'duration': 4.155}, {'text': 'Okay? So, um, ah,', 'start': 3803.375, 'duration': 3.69}, {'text': 'so I hope you understand the lecture notes when hopefully you take a look at them,', 'start': 3807.065, 'duration': 4.305}, {'text': 'ah, just a couple other derivations.', 'start': 3811.37, 'duration': 4.14}, {'text': 'Um, if A [NOISE] is a square matrix.', 'start': 3815.51, 'duration': 5.49}, {'text': "So let's say A is a [NOISE] N,", 'start': 3821.0, 'duration': 3.48}, {'text': 'N by N matrix.', 'start': 3824.48, 'duration': 1.11}, {'text': 'So number of rows equals number of columns.', 'start': 3825.59, 'duration': 2.52}, {'text': "Um, I'm going to denote the trace of A", 'start': 3828.11, 'duration': 3.81}, {'text': '[NOISE] to be equal to [NOISE] the sum of the diagonal entries.', 'start': 3831.92, 'duration': 5.4}, {'text': '[NOISE] So sum of i of A_ii.', 'start': 3837.32, 'duration': 6.7}, {'text': 'And this is pronounced the trace of A, um, and, ah,', 'start': 3844.48, 'duration': 3.985}, {'text': 'and- and you can- you can also write this as', 'start': 3848.465, 'duration': 4.425}, {'text': 'trace operator like the trace function applied to A', 'start': 3852.89, 'duration': 3.72}, {'text': 'but by convention we often write trace of A without the parentheses.', 'start': 3856.61, 'duration': 4.035}, {'text': 'And so this is called the trace of A.', 'start': 3860.645, 'duration': 3.495}, {'text': '[NOISE] So trace just means sum of diagonal entries and,', 'start': 3864.14, 'duration': 4.14}, {'text': 'um, some facts about the trace of a matrix.', 'start': 3868.28, 'duration': 3.075}, {'text': 'You know, trace of A is equal to', 'start': 3871.355, 'duration': 3.495}, {'text': 'the trace of A transpose because if you transpose a matrix,', 'start': 3874.85, 'duration': 3.24}, {'text': "right, you're just flipping it along the- the 45 degree axis.", 'start': 3878.09, 'duration': 3.6}, {'text': 'And so the the diagonal entries actually stay the same when you transpose the matrix.', 'start': 3881.69, 'duration': 3.18}, {'text': 'So the trace of A is equal to trace of A transpose, um,', 'start': 3884.87, 'duration': 4.53}, {'text': 'and then, ah, there-there are some other useful properties of,', 'start': 3889.4, 'duration': 4.8}, {'text': 'um, the trace operator.', 'start': 3894.2, 'duration': 2.28}, {'text': "Um, here's one that I don't want to prove but that you could go home and prove", 'start': 3896.48, 'duration': 4.41}, {'text': 'yourself with a-with a few with- with a little bit of work, maybe not,', 'start': 3900.89, 'duration': 4.245}, {'text': 'not too much which is,', 'start': 3905.135, 'duration': 1.065}, {'text': 'ah, if you define, um,', 'start': 3906.2, 'duration': 3.1}, {'text': 'F of A [NOISE] equals trace of A times B.', 'start': 3909.3, 'duration': 6.71}, {'text': 'So here if B is some fixed matrix, right, ah,', 'start': 3916.01, 'duration': 4.2}, {'text': 'and what F of A does is it multiplies A and', 'start': 3920.21, 'duration': 2.52}, {'text': 'B and then it takes the sum of diagonal entries.', 'start': 3922.73, 'duration': 2.58}, {'text': 'Then it turns out that the derivative with respect to A of F of A is equal to,', 'start': 3925.31, 'duration': 7.755}, {'text': 'um, B transpose [NOISE].', 'start': 3933.065, 'duration': 5.055}, {'text': 'Um, and this is, ah, you could prove this yourself.', 'start': 3938.12, 'duration': 2.34}, {'text': 'For any matrix B, if F of A is defined this way,', 'start': 3940.46, 'duration': 3.165}, {'text': 'the de- the derivative is equal to B transpose.', 'start': 3943.625, 'duration': 2.55}, {'text': 'Um, the trace function or the trace operator has other interesting properties.', 'start': 3946.175, 'duration': 5.565}, {'text': 'The trace of AB is equal to the trace of BA.', 'start': 3951.74, 'duration': 4.5}, {'text': 'Ah, um, you could- you could prove this from past principles,', 'start': 3956.24, 'duration': 3.06}, {'text': "it's a little bit of work to prove,", 'start': 3959.3, 'duration': 1.11}, {'text': 'ah, ah, that- that you,', 'start': 3960.41, 'duration': 1.62}, {'text': 'if you expand out the definition of A and B it should prove', 'start': 3962.03, 'duration': 2.52}, {'text': 'that [NOISE] and the trace of A times B times', 'start': 3964.55, 'duration': 2.79}, {'text': 'C is equal to [NOISE] the trace of C times A times B. Ah,', 'start': 3967.34, 'duration': 5.475}, {'text': 'this is a cyclic permutation property.', 'start': 3972.815, 'duration': 2.265}, {'text': 'If you have a multiply, you know,', 'start': 3975.08, 'duration': 1.605}, {'text': 'multiply several matrices together you can always take one from', 'start': 3976.685, 'duration': 2.745}, {'text': 'the end and move it to the front and the trace will remain the same.', 'start': 3979.43, 'duration': 3.66}, {'text': '[NOISE] And,', 'start': 3983.09, 'duration': 3.96}, {'text': 'um, another one that is a little bit harder to prove is that the trace,', 'start': 3987.05, 'duration': 8.535}, {'text': 'excuse me, the derivative of A trans- of', 'start': 3995.585, 'duration': 3.015}, {'text': 'AA transpose C is [NOISE] Okay.', 'start': 3998.6, 'duration': 9.045}, {'text': 'Yeah. So I think just as- just as for you know, ordinary,', 'start': 4007.645, 'duration': 3.81}, {'text': 'um, calculus we know the derivative of X squared is 2_X.', 'start': 4011.455, 'duration': 4.215}, {'text': 'Right. And so we all figured out that very well.', 'start': 4015.67, 'duration': 1.86}, {'text': 'We just use it too much without- without having to re-derive it every time.', 'start': 4017.53, 'duration': 3.69}, {'text': 'Ah, this is a little bit like that.', 'start': 4021.22, 'duration': 1.8}, {'text': 'The trace of A squared C is,', 'start': 4023.02, 'duration': 2.445}, {'text': 'you know, two times CA.', 'start': 4025.465, 'duration': 1.785}, {'text': "Right. It's a little bit like that but- but with- with matrix notation as there.", 'start': 4027.25, 'duration': 3.99}, {'text': 'So think of this as analogous to D,', 'start': 4031.24, 'duration': 3.24}, {'text': 'DA of A squared C equals 2AC.', 'start': 4034.48, 'duration': 4.62}, {'text': 'Right. But that is like the matrix version of that.', 'start': 4039.1, 'duration': 3.06}, {'text': '[NOISE]', 'start': 4042.16, 'duration': 22.085}, {'text': 'All right. So finally, um,', 'start': 4064.245, 'duration': 7.665}, {'text': "what I'd like to do is take J of Theta and express it in this,", 'start': 4071.91, 'duration': 7.035}, {'text': 'uh, you know, matrix vector notation.', 'start': 4078.945, 'duration': 2.64}, {'text': 'So we can take derivatives with respect to Theta,', 'start': 4081.585, 'duration': 2.145}, {'text': 'and set the derivatives equal to 0,', 'start': 4083.73, 'duration': 1.17}, {'text': 'and just solve for the value of Theta, right?', 'start': 4084.9, 'duration': 2.49}, {'text': 'And so, um, let me just write out the definition of J of Theta.', 'start': 4087.39, 'duration': 5.04}, {'text': 'So J of Theta was one-half sum from i equals 1 through m', 'start': 4092.43, 'duration': 5.055}, {'text': 'of h of x i minus y i squared.', 'start': 4097.485, 'duration': 7.11}, {'text': 'Um, and it turns out', 'start': 4104.595, 'duration': 2.295}, {'text': 'that, um, all right.', 'start': 4106.89, 'duration': 12.975}, {'text': 'It turns out that, um, if it is,', 'start': 4119.865, 'duration': 2.76}, {'text': 'if you define a matrix capital X as follows.', 'start': 4122.625, 'duration': 4.17}, {'text': "Which is, I'm going to take the matrix capital X and take the training examples we have,", 'start': 4126.795, 'duration': 7.86}, {'text': 'you know, and stack them up in rows.', 'start': 4134.655, 'duration': 4.705}, {'text': 'So we have m training examples, right?', 'start': 4140.03, 'duration': 4.18}, {'text': "So so the X's were column vectors.", 'start': 4144.21, 'duration': 2.01}, {'text': "So I'm taking transpose to just stack up the training examples in,", 'start': 4146.22, 'duration': 3.765}, {'text': 'uh, in rows here.', 'start': 4149.985, 'duration': 1.275}, {'text': 'So let me call this the design matrix.', 'start': 4151.26, 'duration': 2.04}, {'text': 'So the capital X called the design matrix.', 'start': 4153.3, 'duration': 2.355}, {'text': 'And, uh, it turns out that if you define X this way,', 'start': 4155.655, 'duration': 5.865}, {'text': 'then X times Theta,', 'start': 4161.52, 'duration': 3.735}, {'text': "there's this thing times Theta.", 'start': 4165.255, 'duration': 3.945}, {'text': 'And the way a matrix vector multiplication', 'start': 4169.2, 'duration': 3.6}, {'text': 'works is your Theta is now a column vector, right?', 'start': 4172.8, 'duration': 3.06}, {'text': 'So Theta is, you know, Theta_0, Theta_1, Theta_2.', 'start': 4175.86, 'duration': 4.11}, {'text': 'So the way that, um,', 'start': 4179.97, 'duration': 2.415}, {'text': 'matrix-vector multiplication works is you multiply', 'start': 4182.385, 'duration': 2.895}, {'text': 'this column vector with each of these in in turn.', 'start': 4185.28, 'duration': 3.075}, {'text': 'And so this ends up being X1 transpose Theta, X2 transpose Theta,', 'start': 4188.355, 'duration': 8.535}, {'text': 'down to X_m transpose Theta,', 'start': 4196.89, 'duration': 6.105}, {'text': 'which is of course just the vector of all of the predictions of the algorithm.', 'start': 4202.995, 'duration': 6.625}, {'text': 'And so if, um,', 'start': 4229.34, 'duration': 2.98}, {'text': 'now let me also define a vector y to be taking all of the,', 'start': 4232.32, 'duration': 6.24}, {'text': 'uh, labels from your training example,', 'start': 4238.56, 'duration': 7.5}, {'text': 'and stacking them up into a big column vector, right.', 'start': 4246.06, 'duration': 3.72}, {'text': 'Let me define y that way.', 'start': 4249.78, 'duration': 1.8}, {'text': 'Um, it turns out that, um,', 'start': 4251.58, 'duration': 5.67}, {'text': 'J of Theta can then be written as one-half of', 'start': 4257.25, 'duration': 6.795}, {'text': 'X Theta minus y transpose X Theta minus y.', 'start': 4264.045, 'duration': 8.43}, {'text': 'Okay. Um, and let me see.', 'start': 4272.475, 'duration': 3.955}, {'text': 'Yeah. Let me just, uh, uh,', 'start': 4276.86, 'duration': 2.83}, {'text': "outline the proof, but I won't do this in great detail.", 'start': 4279.69, 'duration': 2.67}, {'text': 'So X Theta minus y is going to be,', 'start': 4282.36, 'duration': 3.9}, {'text': 'right, so this is X Theta, this is y.', 'start': 4286.26, 'duration': 2.94}, {'text': 'So, you know, X Theta minus y is going to be this vector of h of x1 minus', 'start': 4289.2, 'duration': 7.26}, {'text': 'y1 down to h of xm minus ym, right.', 'start': 4296.46, 'duration': 9.315}, {'text': "So it's just all the errors your learning algorithm is making on the m examples.", 'start': 4305.775, 'duration': 3.615}, {'text': "It's the difference between predictions and the actual labels.", 'start': 4309.39, 'duration': 2.4}, {'text': 'And if you you remember,', 'start': 4311.79, 'duration': 3.03}, {'text': 'so Z transpose Z is equal to sum over i Z squared, right.', 'start': 4314.82, 'duration': 4.74}, {'text': 'A vector transpose itself is a sum of squares of elements.', 'start': 4319.56, 'duration': 3.255}, {'text': 'And so this vector transpose itself is the sum of squares of the elements, right.', 'start': 4322.815, 'duration': 6.915}, {'text': 'So so which is why, uh,', 'start': 4329.73, 'duration': 1.725}, {'text': 'so so the cost function J of Theta is computed', 'start': 4331.455, 'duration': 2.865}, {'text': 'by taking the sum of squares of all of these elements,', 'start': 4334.32, 'duration': 2.67}, {'text': 'of all of these errors,', 'start': 4336.99, 'duration': 1.14}, {'text': 'and and the way you do that is to take this vector,', 'start': 4338.13, 'duration': 2.815}, {'text': 'your X Theta minus y transpose itself,', 'start': 4340.945, 'duration': 3.964}, {'text': 'is the sum of squares of these,', 'start': 4344.909, 'duration': 2.671}, {'text': 'which is exactly the error.', 'start': 4347.58, 'duration': 1.38}, {'text': "So that's why you end up with a,", 'start': 4348.96, 'duration': 2.145}, {'text': 'this as the sum of squares of the, those error terms.', 'start': 4351.105, 'duration': 5.47}, {'text': "Okay. And, um, if some of the steps don't quite make sense,", 'start': 4356.575, 'duration': 8.15}, {'text': "really don't worry about it.", 'start': 4364.725, 'duration': 1.125}, {'text': 'All this is written out more slowly and carefully in the lecture notes.', 'start': 4365.85, 'duration': 3.27}, {'text': 'But I wanted you to have a sense of the, uh,', 'start': 4369.12, 'duration': 2.775}, {'text': 'broad arc of the of the big picture of', 'start': 4371.895, 'duration': 2.835}, {'text': 'their derivation before you go through them', 'start': 4374.73, 'duration': 1.8}, {'text': 'yourself in greater detail in the lecture notes elsewhere.', 'start': 4376.53, 'duration': 3.28}, {'text': 'So finally, what we want to do is take the derivative with respect to Theta of J of Theta,', 'start': 4388.19, 'duration': 8.98}, {'text': 'and set that to 0.', 'start': 4397.17, 'duration': 1.575}, {'text': 'And so this is going to be equal to the derivative of', 'start': 4398.745, 'duration': 4.62}, {'text': "one-half X Theta minus y transpose X Theta minus y. Um, and so I'm gonna,", 'start': 4403.365, 'duration': 8.805}, {'text': "I'm gonna do the steps really quickly, right.", 'start': 4412.17, 'duration': 2.22}, {'text': 'So the steps require some of the little properties of', 'start': 4414.39, 'duration': 3.15}, {'text': 'traces and matrix derivatives I wrote down briefly just now.', 'start': 4417.54, 'duration': 3.165}, {'text': "But so I'm gonna do these very quickly without getting into the details, but, uh,", 'start': 4420.705, 'duration': 4.2}, {'text': 'so this is equal to one-half derivative of Theta of, um.', 'start': 4424.905, 'duration': 5.685}, {'text': 'So take transposes of these things.', 'start': 4430.59, 'duration': 2.22}, {'text': 'So this becomes Theta transpose X transpose minus y transpose.', 'start': 4432.81, 'duration': 5.41}, {'text': 'Right. Um, and then, uh,', 'start': 4438.65, 'duration': 4.405}, {'text': 'kind of like expanding out a quadratic function, right.', 'start': 4443.055, 'duration': 3.705}, {'text': 'This is, you know, A minus B times C minus D. So you can just AC minus AD this and so on.', 'start': 4446.76, 'duration': 5.82}, {'text': "So I'll just write this out.", 'start': 4452.58, 'duration': 3.07}, {'text': 'All right. And so, uh,', 'start': 4468.83, 'duration': 2.32}, {'text': 'what I just did here this is similar to how, you know,', 'start': 4471.15, 'duration': 3.765}, {'text': 'ax minus b times ax minus b,', 'start': 4474.915, 'duration': 3.99}, {'text': 'is equal to a squared x squared minus axb minus bax plus b squared.', 'start': 4478.905, 'duration': 7.14}, {'text': "Is it's kind of, it's just expanding out a quadratic function.", 'start': 4486.045, 'duration': 4.215}, {'text': 'Um, and then the final step', 'start': 4490.26, 'duration': 16.875}, {'text': 'is, yeah, go ahead.', 'start': 4507.135, 'duration': 1.245}, {'text': '[BACKGROUND] Oh, is that right?', 'start': 4508.38, 'duration': 6.84}, {'text': 'Oh yes, thank you. Thank you.', 'start': 4515.22, 'duration': 3.9}, {'text': 'Um, and then the final step is,', 'start': 4519.12, 'duration': 2.595}, {'text': 'you know, for each of these four terms; first,', 'start': 4521.715, 'duration': 2.1}, {'text': 'second, third, and fourth terms,', 'start': 4523.815, 'duration': 2.04}, {'text': 'to take the derivative with respect to Theta.', 'start': 4525.855, 'duration': 3.015}, {'text': 'And if you use some of the formulas I was alluding to over there,', 'start': 4528.87, 'duration': 4.155}, {'text': 'you find that the derivative, um,', 'start': 4533.025, 'duration': 2.475}, {'text': "which which I don't want to show the derivation of,", 'start': 4535.5, 'duration': 2.67}, {'text': 'but it turns out that the derivative is, um,', 'start': 4538.17, 'duration': 2.745}, {'text': 'X transpose X Theta plus X transpose X Theta minus,', 'start': 4540.915, 'duration': 5.805}, {'text': 'um, X transpose y minus X transpose y,', 'start': 4546.72, 'duration': 6.96}, {'text': "um, and we're going to set this derivative.", 'start': 4553.68, 'duration': 4.62}, {'text': 'Actually not, let me just do this.', 'start': 4558.3, 'duration': 1.875}, {'text': 'And so this simplifies to X transpose X Theta minus X transpose y.', 'start': 4560.175, 'duration': 7.035}, {'text': 'And so as as described earlier,', 'start': 4567.21, 'duration': 2.67}, {'text': "I'm gonna set this derivative to 0.", 'start': 4569.88, 'duration': 3.165}, {'text': 'And how to go from this step to that step is using the matrix derivatives,', 'start': 4573.045, 'duration': 3.704}, {'text': 'uh, explained in more detail in the lecture notes.', 'start': 4576.749, 'duration': 2.386}, {'text': 'And so the final step is,', 'start': 4579.135, 'duration': 2.28}, {'text': 'you know, having set this to 0,', 'start': 4581.415, 'duration': 1.56}, {'text': 'this implies that X transpose X Theta equals X transpose y.', 'start': 4582.975, 'duration': 4.32}, {'text': 'Uh, so this is called the normal equations.', 'start': 4587.295, 'duration': 3.385}, {'text': 'And the optimum value for Theta is Theta equals', 'start': 4594.08, 'duration': 6.05}, {'text': 'X transpose X inverse, X transpose y.', 'start': 4600.13, 'duration': 7.915}, {'text': 'Okay. Um, and if you implement this, um,', 'start': 4608.045, 'duration': 6.09}, {'text': 'then, you know, you can in basically one step,', 'start': 4614.135, 'duration': 3.4}, {'text': 'get the value of Theta that corresponds to the global minimum.', 'start': 4617.535, 'duration': 4.38}, {'text': 'Okay. Um, and and and again,', 'start': 4621.915, 'duration': 6.005}, {'text': 'common question I get is one is, well,', 'start': 4627.92, 'duration': 1.41}, {'text': 'what if X is non-invertible?', 'start': 4629.33, 'duration': 1.845}, {'text': 'Uh, what that usually means is you have have redundant features,', 'start': 4631.175, 'duration': 2.775}, {'text': 'uh, that your features are linearly dependent.', 'start': 4633.95, 'duration': 2.655}, {'text': 'Uh, but if you use something called the pseudo inverse,', 'start': 4636.605, 'duration': 2.745}, {'text': "you kind of get the right answer if that's the case.", 'start': 4639.35, 'duration': 1.92}, {'text': 'Although I think the, uh, even more right', 'start': 4641.27, 'duration': 1.62}, {'text': 'answer is if you have linearly dependent features,', 'start': 4642.89, 'duration': 2.19}, {'text': 'probably means you have the same feature repeated twice,', 'start': 4645.08, 'duration': 2.4}, {'text': 'and I will usually go and figure out what features are actually repeated,', 'start': 4647.48, 'duration': 3.075}, {'text': 'leading to this problem.', 'start': 4650.555, 'duration': 1.8}, {'text': 'Okay. All right.', 'start': 4652.355, 'duration': 3.27}, {'text': "Uh, any last questions before- so that so that's the normal equations.", 'start': 4655.625, 'duration': 3.105}, {'text': 'Hope you read through the detailed derivations in the lecture notes.', 'start': 4658.73, 'duration': 2.52}, {'text': 'Any last questions before we break?', 'start': 4661.25, 'duration': 1.605}, {'text': 'Okay.', 'start': 4662.855, 'duration': 0.645}, {'text': '[BACKGROUND]', 'start': 4663.5, 'duration': 13.09}, {'text': 'Oh, yeah.', 'start': 4676.59, 'duration': 0.69}, {'text': 'How do you choose a learning rate?', 'start': 4677.28, 'duration': 1.11}, {'text': "It's, it's, it's quite empirical, I think.", 'start': 4678.39, 'duration': 2.16}, {'text': 'So most people would try different values,', 'start': 4680.55, 'duration': 1.92}, {'text': 'and then just pick one. All right.', 'start': 4682.47, 'duration': 3.165}, {'text': "I think let's let's break.", 'start': 4685.635, 'duration': 1.185}, {'text': 'If, if people have more questions,', 'start': 4686.82, 'duration': 1.47}, {'text': "when the TAs come up, we're going to keep taking questions.", 'start': 4688.29, 'duration': 2.04}, {'text': "Well, let's break for the day. Thanks everyone.", 'start': 4690.33, 'duration': 2.92}]